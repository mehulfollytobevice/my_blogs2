[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hi 👋, I’m Mehul",
    "section": "",
    "text": "Welcome to Thoughts, Code and Mischief. A place to find interesting stuff on AI, ML and life.\n\nAbout me:\nI am a perpetual learner on a journey through the known and unknown lands of machine learning, data science and natural language processing. Currently, I am a graduate student at Northeastern University, Boston pursuing my master’s degree in data science. I completed my undergraduate degree in Computer Science and Engineering from Vellore Institute of Technology, Vellore. In the past, I have worked on multiple research papers and projects tackling problems in the domains of ML, DL and NLP. I aspire to become a successful data scientist and create data driven solutions for solving problems at a global scale."
  },
  {
    "objectID": "posts/2024-05-15-pricealchemy.html",
    "href": "posts/2024-05-15-pricealchemy.html",
    "title": "MLOps in Action - How to Implement an End-to-End Pipeline for Your Project",
    "section": "",
    "text": "As data scientists, we often find ourselves in the familiar, comfortable realms of Jupyter notebooks and Python IDEs. However, there comes a time when we must step out of our natural habitats and present our work to a broader audience, encompassing various stakeholders. This is where MLOps (Machine Learning Operations) comes into play, bridging the gap between data science and operational processes. In this blog post, I’ll take you through an end-to-end MLOps pipeline based on a project I’ve recently completed. We’ll talk about each stage of the pipeline, exploring how to design processes with the goal of creating robust, scalable machine learning systems. Whether you’re a data scientist looking to operationalize your models or an engineer aiming to optimize ML workflows, this blog post will provide practical insights and strategies to elevate your machine learning projects."
  },
  {
    "objectID": "posts/2024-05-15-pricealchemy.html#project-introduction",
    "href": "posts/2024-05-15-pricealchemy.html#project-introduction",
    "title": "MLOps in Action - How to Implement an End-to-End Pipeline for Your Project",
    "section": "Project Introduction:",
    "text": "Project Introduction:\nI recently completed an end-to-end machine learning project called Price Alchemy, which we will use to explore the various facets of MLOps in practice. You can check out the project here.\nPrice Alchemy is a price suggestion system for second-hand e-commerce platforms like Mercari or EBay. Imagine you’re a seller on this platform: you need to list an item at a price that’s attractive to buyers but still profitable for you. Pricing too high could deter buyers, while pricing too low could result in a loss. The goal of Price Alchemy is to suggest an optimal price that balances these constraints, maximizing seller profit while ensuring buyer interest.\nTo achieve this, we developed a model that integrates textual data about the product with tabular data. This approach allows us to predict the price of an item by considering multiple variables such as item category, brand, and description. By leveraging this comprehensive data, the system can make accurate and practical price suggestions tailored to the dynamic nature of the second-hand market.\nThis project uses the Mercari Price Suggestion Challenge dataset from Kaggle."
  },
  {
    "objectID": "posts/2024-05-15-pricealchemy.html#project-structure",
    "href": "posts/2024-05-15-pricealchemy.html#project-structure",
    "title": "MLOps in Action - How to Implement an End-to-End Pipeline for Your Project",
    "section": "Project Structure:",
    "text": "Project Structure:\nPerusing the GitHub repository tells you that the project contains several setup files and sub-folders for organizing different parts of the project.\nLet’s understand the file organization:\n\ndata/: This directory contains the raw and processed data. Raw data files (train.csv and test.csv) are stored in this directory, along with processed data files generated at the end of the data-preprocessing pipeline.\nnotebooks/: Jupyter notebooks play a crucial role in providing a playground for developing code related to exploratory data analysis, data preprocessing, model development, and evaluation. All the Jupyter notebooks developed as a part of the project are stored here.\ndags/: Every production level ML system uses a workflow orchestration platform like AirFlow to automate the execution of various pipelines (data preprocessing, training, testing, re-training, etc). In this project, we employ Airflow to take care of this function. This directory contains the source code for Airflow DAGs that automate parts of the machine learning pipeline. It also contains the source code for the price_alchemy module which is used for performing data loading, validation and data preprocessing, modeling, and model evaluation. The price_alchemy module can be easily installed in the working directory using pip.\ntests/: An essential but often ignored part of any good project are the tests that are used to validate the proper functioning of the source code. This directory contains unit tests, integration tests, or any other tests relevant to the project.\nmodels/: Saved model files generated during model training are stored in this directory.\n.github/workflows: This folder contains YAML files defining workflows for automating various tasks related to the project. In the context of this project, GitHub workflows are used to automate the testing and validation of code changes. This involves running unit tests, integration tests, and code quality checks whenever new code is pushed to the repository.\nrequirements.txt: A text file listing the Python package dependencies required to run the project.\nREADME.md: Documentation providing an overview of the project, instructions for setup and usage, and any additional information relevant to users and contributors."
  },
  {
    "objectID": "posts/2024-05-15-pricealchemy.html#tools-and-project-flowchart",
    "href": "posts/2024-05-15-pricealchemy.html#tools-and-project-flowchart",
    "title": "MLOps in Action - How to Implement an End-to-End Pipeline for Your Project",
    "section": "Tools and Project Flowchart:",
    "text": "Tools and Project Flowchart:\nHaving understood the project’s file structure, let’s dive into the tools utilized and the overall architecture of the project. The following flow chart details each stage of the project’s workflow, illustrating how data flows through various tools and components, from data ingestion and preprocessing to model training, evaluation, and deployment.\n\n\n\nProject Architecture\n\n\nLet’s pause for a moment to talk about the diverse array of tools employed and their pivotal roles in the overall architecture:\n\nBigQuery Database: Imagine starting your machine learning project with a treasure trove of data. That’s what BigQuery is! It’s the ultimate data warehouse in the cloud, capable of handling massive datasets with ease. BigQuery helps us to perform lightning-fast SQL queries on the data, making it the backbone for storing and managing the vast amounts of information needed for the machine learning models. All of the training and validation data is stored in BigQuery for ease of access and scalability.\nJupyter Notebooks: For any data scientist, Jupyter Notebooks are the playground where creativity meets data. We use Notebooks for experimentation, developing parts of the source code, performing EDA, and documenting the findings in one place.\nGitHub: GitHub is a platform for collaboration and versioning control. All of the code developed in the project is stored and maintained on GitHub. Furthermore, every time new code is pushed to GitHub, a workflow runs a series of integration tests to ensure everything works the way it’s supposed to.\nApache Airflow: Airflow is like the conductor of an orchestra, ensuring that each part of the ML pipeline runs in harmony. It automates the complex workflows involved in data validation, preprocessing, model training, and evaluation.\nMLfLow: A critical aspect of any machine learning project is experimentation. Exploring various data preprocessing techniques, models, and hyper-parameters is essential to identify the optimal solution that meets all the project’s requirements. And, this process of exploration can get chaotic very quickly. MLflow brings order to this chaos by tracking experiments, models, and parameters.\nHyperopt: Tuning a model’s hyper-parameters can be very challenging, so we use Hyperopt to simplify the process. By automating the search for optimal hyper-parameters, Hyperopt ensures that the model reaches its highest accuracy and efficiency.\nDocker (Deployment): Consistency is key when moving models from development to production. Docker ensures that our model runs smoothly across different environments by containerizing it with all its dependencies. It’s like packaging your model into a self-contained unit that can be easily transported and deployed anywhere.\nFastAPI (Prediction App/UI): To make the model accessible to users, we need an intuitive user interface or a robust API. FastAPI, a modern and high-performance web framework for building APIs with Python, is an excellent choice. It enables us to serve model predictions with minimal latency, ensuring a responsive user experience.\nELK Stack: Elasticsearch, Logstash, Kibana (Model Monitoring): Deploying a model is not the end; it’s just the beginning. Model monitoring ensures that our model continues to perform well in the real world. The ELK Stack – Elasticsearch for searching and analyzing data, Logstash for processing logs, and Kibana for visualizing data – provides a robust solution for monitoring the model."
  },
  {
    "objectID": "posts/2024-05-15-pricealchemy.html#stages-of-a-ml-project-lifecycle",
    "href": "posts/2024-05-15-pricealchemy.html#stages-of-a-ml-project-lifecycle",
    "title": "MLOps in Action - How to Implement an End-to-End Pipeline for Your Project",
    "section": "Stages of a ML project lifecycle:",
    "text": "Stages of a ML project lifecycle:\nDeveloping a machine learning product involves multiple stages, each crucial for ensuring the final solution is effective, scalable, and aligned with business objectives. To make this process smooth, it’s important to follow a structured template that guides the development from starting to finish. In general, the following is a good template to stick to:\n\nProduct (What & Why) -&gt; Engineering (How) -&gt; Project (Who & When)\n\n\nProduct Management (What and Why):\nUnderstanding the motivation behind the product and defining clear objectives and key results (OKRs) is the first step in the process. This involves answering:\n\nWhat is the motivation behind the product?\n\nBackground: Defining the target customer and their goals is essential. It’s important to clarify:\n\nWho is the customer we are targeting?\nWhat is the main goal for the customer?\nWhat are the obstacles preventing the customer from achieving their goal?\nWhat can make the customer’s job easier?\n\nValue Proposition: To create a compelling product, you need to focus on reducing customer pains and creating advantages. Key questions include:\n\nWhat needs to be built to help the customer reach their goal?\nHow will the product reduce customer pains?\nHow will the product create advantages for the customer?\n\n\nWhat are the objectives and key results (OKRs) for the product?\n\nObjective: Defining specific objectives helps focus the product development process. Consider:\n\nWhat are the key objectives we want to focus on for the product?\nHow does the customer interact with the product?\n\nSolution: Developing a solution involves identifying core features, integration points, alternatives, and constraints. Addressing these questions is crucial:\n\nWhat core features are required to meet our objectives?\nWhat are the integration points for the solution?\nWhat are the alternatives to the proposed solution?\nWhat are the constraints we need to consider?\nWhat is out-of-scope for this solution?\n\nFeasibility: Assessing the feasibility of the solution ensures that it can be practically implemented. Important questions include:\n\nHow feasible is our solution?\nDo we have the required resources (data, $, team) to deliver the solution?\n\n\n\nBy systematically addressing these questions, we can effectively scope and guide the development of machine learning products. This ensures that out end-product is customer-centric, feasible, and valuable.\n\n\nEngineering (How):\nIn this stage, the focus shifts from defining goals and understanding customer needs to figuring out how to implement the solution. This involves planning and executing the technical aspects, such as data management, labeling processes, core feature development, and system integration.\n\nData:\nAs the old adage in data science goes, “Garbage in, garbage out.” If your data quality is poor, your machine learning model and everything that follows will also be compromised. Ensuring the integrity and quality of your data is paramount to building effective machine learning solutions. While picking a source of data or collecting data, we should try to answer the following questions:\n\nTraining data: What we use to train our machine learning model.\n\nDo we have access to input labels for the data?\nDo we know the origin of the data and the schema it follows?\nWas there any kind of sampling done to generate this dataset?\nAre we introducing any data leaks?\n\nProduction data: The unseen data that is passed into the machine learning model after deployment.\n\nIs the data accessible timely and regularly?\nHow do we trust that this data follows the same distribution as the training data?\n\nLabelling and Feature Engineering: Often, we have to label input data to make it suitable for a supervised learning task.\n\nHow do we decide on a label definition that is appropriate with respect to the context of the problem?\nHow do we design the labelling process, can it be done in a semi-supervised fashion?\nWhich features best capture the essential information needed for the task at hand?\n\n\nLet’s try to answer some of these questions in the context of Price Alchemy:\nDo we have access to input labels for the data?\n\nYes, the dataset includes the target variable price, which is the label for supervised learning. This makes it much easier to train a machine learning model by eliminating the labelling step.\n\nDo we know the origin of the data and the schema it follows?\n\nYes, the data comes from Kaggle’s Mercari Price Suggestion Challenge. The schema includes features such as item condition, category, brand, and item description.\n\nHow do we trust that this data follows the same distribution as the training data?\n\nThe price_alchemy module contains a python class (data_validation.py) which allows to apply a range of data quality checks on the input data. From basic checks like validating the minimum and maximum value of features to validating the z-score of the price variable, all is covered in this python class.\n\nHow do we decide on a label definition that is appropriate with respect to the context of the problem?\n\nThe feature price is suitable for the problem as it directly aligns with the business objective of estimating the optimal value of items for sale.\n\nWhich features best capture the essential information needed for the task at hand?\n\nKey features include item condition, category, brand, and item description, as they are directly relevant to determining the price of an item.\n\n\n\nEvaluation:\nEstablishing evaluation criteria early on provides a well-defined roadmap, guiding us through the complexities of data analysis and model development. It sets a quantifiable goal, allowing us to measure progress objectively and make informed decisions throughout the project.\nHere are a few points that can help us in designing a good evaluation procedure:\n\nDeciding on a metric: Translating a qualitative business goal into a quantifiable metric is quite hard. The following questions can be helpful deciding which metric to use.\n\nWhat kind of problem is it? Classification or regression?\nDoes the metric capture the desired aspect of model performance?\nWhat are the implications of false positives and false negatives?\nHow sensitive is the metric to changes in model predictions?\n\nIntrinsic Evaluation vs. Extrinsic Evaluation: Intrinsic evaluation involves measuring the model’s performance solely based on its output, independent of any external context. On the other hand, extrinsic evaluation evaluates the model within the context of its application or task, considering how well it performs in real-world scenarios.\n\nAre there any standardized benchmarks or datasets commonly used for intrinsic evaluation in the field?\nHow does the model perform on downstream tasks or applications?\n\nOnline evaluation vs offline evaluation: Offline evaluation involves analyzing historical data sets to assess model performance in controlled environments. Conversely, online evaluation entails evaluating models in real-time or near real-time settings, often using streaming data.\n\nDoes the model need to achieve a certain level of performance before it can be deployed?\nHow important is it to assess the model’s performance in dynamic, real-world conditions, including user interactions and changing environments?\nWhat level of immediacy is required for evaluating model performance and making decisions based on evaluation results?\n\n\nIn the context of Price Alchemy:\nWhat kind of problem is it? Classification or regression?\n\nIn this project, we are dealing with a regression problem.\n\nDoes the metric capture the desired aspect of model performance?\n\nMean Absolute Error (MAE) or Root Mean Squared Error (RMSE) would capture how accurately the model predicts prices, reflecting the average prediction error in the same units as the price.\n\nHow sensitive is the metric to changes in model predictions?\n\nRMSE is more sensitive to outliers than MAE, making it suitable if large errors are particularly undesirable.\n\nHow important is it to assess the model’s performance in dynamic, real-world conditions, including user interactions and changing environments?\n\nIt is crucial to assess performance in real-world conditions to capture effects of user behavior and market trends, which can differ from historical data.\n\n\n\nModeling:\nHow we model our problem can vary greatly based on the context, computational complexity, response time, etc. But, there are some general guidelines that can be followed:\n\nEnd-to-End utility: The end result from every iteration should deliver minimum end-to-end utility so that we can benchmark iterations against each other and plug-and-play with the system.\nRules before ML: Before employing a ML based system, it is helpful to use a deterministic rules based approach as a baseline.\nAugmentation is always better: Augmenting the decision making process for a human is often better than automating the final decision process. This helps in keeping a human in the loop in case the system gives an unexpected result.\nEarly releases for validation: Not all early releases need to be end-user facing. Some can be used for internal validation, feedback, data collection, etc.\nExperiment! Experiment! Experiment!: All models should be well tested and evaluated, so that we can objectively benchmark different approaches. This involves testing the code, validating the data quality and logging all the model runs with the hyper parameters used.\n\nIn the context of Price Alchemy:\nEnd-to-End Utility:\n\nFor Price Alchemy, this means each model version must predict prices accurately enough to be deployable in a testing or production environment.\n\nRules Before ML:\n\nBefore implementing a ML based pricing system, we can start with a deterministic rules-based approach as a baseline. This could involve setting pricing rules based on factors such as item_condition, brand, item description keywords, and market trends.\n\nAugmentation is always better:\n\nRather than fully automating the pricing decision process, augmenting the decision-making process for human oversight is significantly more beneficial. This involves developing a pricing model that provides recommendations or insights to sellers, who can then make informed decisions based on their expertise and intuition.\n\nExperiment! Experiment! Experiment!:\n\nIn this project, we make sure to thoroughly test each version of our code using tools like mypy and pytest. Additionally, we use MLFlow to log each training or cross-validation run of our model. This helps us keep a record of all the settings we use (the hyper parameters) , and the results we get (evaluation metrics). We also keep track of the type of preprocessed data we use to train the model, so we can understand how it affects the results.\n\n\n\n\nProject Management (Who and When):\nAt the end of the day, it boils down to organizing all the work into reasonable timelines and allocating specific responsibilities to teams/individuals so that the end-product can be delivered on time. Here are a few points to keep in mind.\n\nTeam Involvement: It’s crucial to identify and involve all relevant team members from the start. This includes product managers, data engineers, machine learning engineers, DevOps, UI/UX designers, accessibility experts, and site reliability engineers. Each team has specific roles and responsibilities, ensuring that all aspects of the project are covered comprehensively.\nDeliverables: Breaking down objectives into clear deliverables is essential. Each deliverable should specify the task, the contributors, dependencies, acceptance criteria, and status. This creates a detailed checklist for teams to prioritize their tasks.\nObjective Prioritization: Objectives need to be categorized by priority and assigned to specific releases.\nTimelines and Milestones: Setting a timeline for the project is critical. This involves conducting exploration studies, pushing iterations to development, staging, and eventually production. The timeline should be transparent and allow for adjustments based on feasibility studies and stakeholder feedback.\nIterative Engagement: Continuous engagement with the product and engineering teams is necessary to ensure the right product is built effectively. Iterative feedback and adjustments help in refining the product continuously."
  },
  {
    "objectID": "posts/2024-05-15-pricealchemy.html#conclusion",
    "href": "posts/2024-05-15-pricealchemy.html#conclusion",
    "title": "MLOps in Action - How to Implement an End-to-End Pipeline for Your Project",
    "section": "Conclusion:",
    "text": "Conclusion:\nImplementing an end-to-end MLOps pipeline is a transformative journey that bridges the gap between data science and operational processes. Through our exploration of the Price Alchemy project, we’ve learned how each part of the MLOps pipeline—from data collection and cleaning to model training, deployment, and monitoring— is essential in creating an accurate, reliable machine learning solution.\nUsing tools like BigQuery, Jupyter Notebooks, Apache Airflow, and Docker, along with following best practices in product management and project management, we can build a smooth workflow. This systematic approach not only enhances the efficiency of our operations but also ensures that our models deliver real value to end-users.\nFor data scientists and ML engineers, adopting an MLOps framework offers a structured path to transitioning from experimental notebooks to production-ready systems. It fosters collaboration across multidisciplinary teams, allowing for continuous improvement and facilitates sharing of work with a broader audience.\nIn summary, MLOps is more than a set of practices; it’s a mindset that embraces the full lifecycle of machine learning projects. By setting up a solid MLOps pipeline, you can make sure your models are practical tools that drive business success. Whether you’re just starting out in data science or looking to improve your ML workflows, integrating MLOps into your projects will take them to the next level."
  },
  {
    "objectID": "posts/2024-05-15-pricealchemy.html#references",
    "href": "posts/2024-05-15-pricealchemy.html#references",
    "title": "MLOps in Action - How to Implement an End-to-End Pipeline for Your Project",
    "section": "References:",
    "text": "References:\n\nMade with ML Product Design\nMercari Dataset\nPrice Alchemy Repository\nML with Ramin\nPeople+ AI Guidebook\nKey requirements for an MLOps foundation"
  },
  {
    "objectID": "posts/2021-04-12-preprocess.html",
    "href": "posts/2021-04-12-preprocess.html",
    "title": "The essentials of data preprocessing for tabular data",
    "section": "",
    "text": "If you ask data scientists which part of the ML pipeline takes the most amount of time, they will probably tell you that it’s the data preprocessing stage. Ensuring your data is in the right form before you dump it into your ML/DL model is of paramount importance. If you feed in garbage to your model, you will get garbage as the output. In this blog post we will see some of the essential techniques that you can use to preprocess your data properly.\nBut first, we need to get some data."
  },
  {
    "objectID": "posts/2021-04-12-preprocess.html#downloading-dataset-from-kaggle",
    "href": "posts/2021-04-12-preprocess.html#downloading-dataset-from-kaggle",
    "title": "The essentials of data preprocessing for tabular data",
    "section": "Downloading dataset from Kaggle",
    "text": "Downloading dataset from Kaggle\nThis notebook is built using Google Colab. It is a notebook server provided by Google for free. You can also use other services to run the code below but you will have to figure out how to get the dataset. The dataset that we use here is present on Kaggle and you can directly download it from here.\nIn this notebook, we are going to download the dataset from Kaggle into Google Colab and store it in a directory in our Google Drive. Storing your data in the Drive saves you from the trouble of downloading the dataset every time you start a new session in Colab.\nFor further guidance read this wonderful article by Mrinali Gupta: How to fetch Kaggle Datasets into Google Colab\nSo let’s get to it!\nFirst, we need to mount our google drive so that we can access all the folders in the drive.\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nThen we will using the following code to provide a config path for the Kaggle Json API\n\nimport os\nos.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/kaggle/StudentPerformance\"\n\nWe will change our current directory to where we want the dataset to be downloaded\n\n%cd /content/gdrive/My Drive/kaggle/StudentPerformance\n\n/content/gdrive/My Drive/kaggle/StudentPerformance\n\n\nNow we can download the dataset from kaggle\n\n!kaggle datasets download -d spscientist/students-performance-in-exams\n\nDownloading students-performance-in-exams.zip to /content/gdrive/My Drive/kaggle/StudentPerformance\n  0% 0.00/8.70k [00:00&lt;?, ?B/s]\n100% 8.70k/8.70k [00:00&lt;00:00, 1.14MB/s]\n\n\nLet’s unzip the files\n\n!unzip \\*.zip  && rm *.zip\n\nArchive:  students-performance-in-exams.zip\n  inflating: StudentsPerformance.csv  \n\n\nWhat files are present in the current directory?\n\n!ls\n\nkaggle.json  StudentsPerformance.csv\n\n\nYou can see that there is a “StudentsPerformance.csv” file present in the directory. That is our dataset."
  },
  {
    "objectID": "posts/2021-04-12-preprocess.html#exploring-the-data",
    "href": "posts/2021-04-12-preprocess.html#exploring-the-data",
    "title": "The essentials of data preprocessing for tabular data",
    "section": "Exploring the data",
    "text": "Exploring the data\nBefore we apply any preprocessing steps to the data, we need to know what kind of data the dataset contains. Is it textual data? Is it numerical data? Are there any dates present? What about geographic locations?\nThere are a lot of questions we can ask about the data in out dataset. So before we move further, we need to get a sense of what hidden knowledge our dataset contains.\nIn the code cells below you will see some of the most common steps you can apply to gather information about your data.\n\nimport pandas as pd\ndf=pd.read_csv(\"StudentsPerformance.csv\")\n\nFirst 5 rows of the dataset Seeing the first and last few rows can be really helpul in creating a mental picture about the data. It also allows you to map out the possible roadblocks you are going to face in acheiving your end goal.\n\ndf.head(5)\n\n\n\n\n\n\n\n\n\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\n\n\n0\nfemale\ngroup B\nbachelor's degree\nstandard\nnone\n72\n72\n74\n\n\n1\nfemale\ngroup C\nsome college\nstandard\ncompleted\n69\n90\n88\n\n\n2\nfemale\ngroup B\nmaster's degree\nstandard\nnone\n90\n95\n93\n\n\n3\nmale\ngroup A\nassociate's degree\nfree/reduced\nnone\n47\n57\n44\n\n\n4\nmale\ngroup C\nsome college\nstandard\nnone\n76\n78\n75\n\n\n\n\n\n\n\n\nLast 5 rows of the dataset\n\ndf.tail(5)\n\n\n\n\n\n\n\n\n\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\n\n\n995\nfemale\ngroup E\nmaster's degree\nstandard\ncompleted\n88\n99\n95\n\n\n996\nmale\ngroup C\nhigh school\nfree/reduced\nnone\n62\n55\n55\n\n\n997\nfemale\ngroup C\nhigh school\nfree/reduced\ncompleted\n59\n71\n65\n\n\n998\nfemale\ngroup D\nsome college\nstandard\ncompleted\n68\n78\n77\n\n\n999\nfemale\ngroup D\nsome college\nfree/reduced\nnone\n77\n86\n86\n\n\n\n\n\n\n\n\nInformation about data type of columns and null values  Knowing the data type of each column is crucial in choosing the right preprocessing step for that column as well as understanding what the values in the column represent.\nAnother crucial piece of information is the number of non-null values. It helps you in deciding which columns need imputation.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   gender                       1000 non-null   object\n 1   race/ethnicity               1000 non-null   object\n 2   parental level of education  1000 non-null   object\n 3   lunch                        1000 non-null   object\n 4   test preparation course      1000 non-null   object\n 5   math score                   1000 non-null   int64 \n 6   reading score                1000 non-null   int64 \n 7   writing score                1000 non-null   int64 \ndtypes: int64(3), object(5)\nmemory usage: 62.6+ KB\n\n\nChecking if the dataset has null values  In the last cell, we saw how you can see if there are null values in your dataset. Below is another method to confirm if your data has any null/ missing values. As you can see below, this dataset does not have any null values.\n\nfor i in list(df.columns):\n  bool_series=pd.isnull(df[i])\n  print(\"Column:{} has {} null values.\".format(i,df[bool_series].shape[0]))\n\nColumn:gender has 0 null values.\nColumn:race/ethnicity has 0 null values.\nColumn:parental level of education has 0 null values.\nColumn:lunch has 0 null values.\nColumn:test preparation course has 0 null values.\nColumn:math score has 0 null values.\nColumn:reading score has 0 null values.\nColumn:writing score has 0 null values.\n\n\nDescribing the numerical columns in the dataset If you are applying regression or even classification, knowing the summary statistics might help you in deciding how you want to handle the numerical features. Maybe you have to apply some transformations before you apply regression. Maybe the numerical features can be dropped in case they do not contribute much.\n\ndf.describe()\n\n\n\n\n\n\n\n\n\nmath score\nreading score\nwriting score\n\n\n\n\ncount\n1000.00000\n1000.000000\n1000.000000\n\n\nmean\n66.08900\n69.169000\n68.054000\n\n\nstd\n15.16308\n14.600192\n15.195657\n\n\nmin\n0.00000\n17.000000\n10.000000\n\n\n25%\n57.00000\n59.000000\n57.750000\n\n\n50%\n66.00000\n70.000000\n69.000000\n\n\n75%\n77.00000\n79.000000\n79.000000\n\n\nmax\n100.00000\n100.000000\n100.000000\n\n\n\n\n\n\n\n\nHow many unique values does each column have  You might want to know how many unique values each column has. This is helpful when you have a big dataset and you are thinking of generating more features from the existing features. This is also important when you are dealing with cases where the Curse of Dimensionality becomes relevant.\n\ndf.nunique()\n\ngender                          2\nrace/ethnicity                  5\nparental level of education     6\nlunch                           2\ntest preparation course         2\nmath score                     81\nreading score                  72\nwriting score                  77\ndtype: int64"
  },
  {
    "objectID": "posts/2021-04-12-preprocess.html#the-essential-preprocessing-techniques",
    "href": "posts/2021-04-12-preprocess.html#the-essential-preprocessing-techniques",
    "title": "The essentials of data preprocessing for tabular data",
    "section": "The essential preprocessing techniques",
    "text": "The essential preprocessing techniques\nIn this section, we will cover the essential preprocessing techniques you can apply to your data before feeding it into your model. This is by no means an exhaustive list of techniques you can apply, but this covers the most common techinques applied in the ML industry.\nThe order in which we apply these techniques is very important since each preprocessing step transforms the data such that it may become incompatible for another preprocessing step.\nWe are going to apply our preprocessing techniques in the following order : 1. Label Encoding (if needed) 2. One-hot Encoding 3. Imputation 5. Normalization or Standardization\n\nLabel Encoding\nWe saw in the previous section that our data cantains columns which have string values. Although, we can understand what these string values represent, a machine does not. So , to make these values machine-readable we have to find a way to represent them numerically. Label Encoding is one such method of accomplishing this.\nIn Label Encoding, we assign a unique integer value to each class in the data. This means that the gender column in our dataset will be encoded like so:\n\n\n\nOriginal values\n\n\nMale\n\n\nFemale\n\n\n\nLabel Encoded values\n\n\n0\n\n\n1\n\n\n\nLet’s see this in action. We are going to label encode the following columns in our dataset:\n\ngender\n\nrace/ethnicity\n\nparental level of education\n\nlunch\ntest preparation course\n\n\nfrom sklearn.preprocessing import LabelEncoder\ndata=df.copy() # creating a copy of our data since we do not want to change the original dataset\nfor i in list(data.columns)[:5]:\n  data[i]=LabelEncoder().fit_transform(data[i])\ndata.head(5)\n\n\n\n\n\n\n\n\n\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\n\n\n0\n0\n1\n1\n1\n1\n72\n72\n74\n\n\n1\n0\n2\n4\n1\n0\n69\n90\n88\n\n\n2\n0\n1\n3\n1\n1\n90\n95\n93\n\n\n3\n1\n0\n0\n0\n1\n47\n57\n44\n\n\n4\n1\n2\n4\n1\n1\n76\n78\n75\n\n\n\n\n\n\n\n\nAlthough Label Encoding can be useful in many cases, there is a caveat. An algorithm will not be able to differentiate between a numerical variable and a label encoded variable. Due to this limitation, the values in the label encoded columns might be misunderstood. So in our gender column, ‘1’ might be given a higher priority than ‘0’ even when no numerical relation exists between the two classes.\nDue to this limitation, we need to find a better way of representing our categorical variables in a numerical form.\n\n\nOne-hot encoding\nDue to the shortcomings of Label Encoding, we cannot apply it to transform categorical variables into numerical values. Instead, we use One-Hot Encoding. In One-Hot Encoding, we take the n categories in a column and create n or n-1 columns from them. The new columns contain only binary values (0 or 1). So, if our gender column is one-hot encoded, then we will have two new columns: Male and Female. The values in these columns will be 0 (Male column) and 1 (Female column) if a row earlier had ‘female’ as the gender and vice versa.\nLet’s see how to implement this.\n\ndf=pd.get_dummies(data=df,drop_first=True) #drop_first can also be False\ndf.head(5) \n\n\n\n\n\n\n\n\n\nmath score\nreading score\nwriting score\ngender_male\nrace/ethnicity_group B\nrace/ethnicity_group C\nrace/ethnicity_group D\nrace/ethnicity_group E\nparental level of education_bachelor's degree\nparental level of education_high school\nparental level of education_master's degree\nparental level of education_some college\nparental level of education_some high school\nlunch_standard\ntest preparation course_none\n\n\n\n\n0\n72\n72\n74\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n\n\n1\n69\n90\n88\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n2\n90\n95\n93\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n\n\n3\n47\n57\n44\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n76\n78\n75\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n\n\n\n\n\n\n\n\nIn the above code, the function has a parameter drop_first. What does this mean? In one-hot encoding, we usually drop one of the columns created for each categorical variable to avoid high correlation among the features. But, in many cases dropping columns can have a negative impact on the model performance. So it is always better to experiment and see what works in your case.\nOne-hot encoding is applied in almost all cases where we have to deal with categorical variables. But when the number of categories in a column gets too big, we cannot use this technique since the resulting dataset might be very large and difficult to handle. In this case, you might want to consider other alternatives like dropping the high-cardinality columns, using label encoding, using dimensionality reduction techniques.\n\n\nImputation\nDealing with missing data is another important data preprocessing step. Missing values can have huge impacts on how your model performs. If a significant portion of values in a column are missing then you might consider dropping that column. But,dropping columns might actually result in leaving out essential information.So, we apply imputation.\nIn imputation, we replace missing data with substituted values. Here, we will dicuss some of the common ways in which imputation is done: * Replace by 0: Sometimes replacing numerical values with 0 can work. Suppose you have an Age column in your dataset. Filling 0 in the places where age is missing might not affect the model’s accuracy. * Replace by mean: You can also take the mean of all the values in the column and use that to fill the missing places. This is the most common imputation approach. But it is very sensitive to outliers. * Replace by most_frequent: You can replace the missing values with the most frequent value. This can work for both categorical and numerical data. * Replace using custom function: If you know how a particular column was generated then you can use that process to fill the missing values. Usually, this approach is not applicable since the data is downloaded from elsewhere.\nIn our dataset, we do not have any missing values so we do not need to apply imputation.\nFor futher guidance: Imputation of missing values\n\n\nStandardization\nEven after converting our data into a machine readable format, our work is not done. In any dataset, you will have parameters that our measured in different units. For example, you might have Time (measured in hours) and Distance (measured in miles).The values of these parameters will have different distributions and different min/max values. You cannot combine these different parameters using a ML model without taking into account their measurement units.So, we use standardization and normalization.\nBoth of these techniques transform the data in a way such that it either becomes dimensionless (in terms of measurement units) or the parameters end up having similar distributions.\nThe biggest difference between standardization and normalization is as follows: * Standardization typically rescales the values to have a mean of 0 and a standard deviation of 1 (unit variance). * Normalization typically rescales the values into a range of [0,1].\nBecause of the difference in the way they transform the values we get a different output in each case.\nLet’s scale our data.\n\nfrom sklearn.preprocessing import StandardScaler\ndata=df.copy()\ndata=StandardScaler().fit_transform(data)\ndata[:3]\n\narray([[ 0.39002351,  0.19399858,  0.39149181, -0.96462528,  2.0647416 ,\n        -0.68441857, -0.59583014, -0.40347329,  2.73396713, -0.49374193,\n        -0.2503982 , -0.54036068, -0.4669334 ,  0.74188112,  0.74674788],\n       [ 0.19207553,  1.42747598,  1.31326868, -0.96462528, -0.4843221 ,\n         1.46109419, -0.59583014, -0.40347329, -0.36576885, -0.49374193,\n        -0.2503982 ,  1.85061578, -0.4669334 ,  0.74188112, -1.33914006],\n       [ 1.57771141,  1.77010859,  1.64247471, -0.96462528,  2.0647416 ,\n        -0.68441857, -0.59583014, -0.40347329, -0.36576885, -0.49374193,\n         3.99363901, -0.54036068, -0.4669334 ,  0.74188112,  0.74674788]])\n\n\nNow let’s look at normalization.\n\n\nNormalization\nNormalization typically rescales the values into a range of [0,1]. As you will see below, there is a notable difference between the output of standardization and normalization. Normalization will not transform the values of your categorical/one-hot encoded variables in any way. On the other hand, standardization transforms all the columns in the dataset.\nSo, let’s normalize our data.\n\nfrom sklearn.preprocessing import MinMaxScaler\ndata=df.copy()\ndata=MinMaxScaler().fit_transform(data)\ndata[:3]\n\narray([[0.72      , 0.6626506 , 0.71111111, 0.        , 1.        ,\n        0.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        , 0.        , 1.        , 1.        ],\n       [0.69      , 0.87951807, 0.86666667, 0.        , 0.        ,\n        1.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 1.        , 0.        , 1.        , 0.        ],\n       [0.9       , 0.93975904, 0.92222222, 0.        , 1.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        1.        , 0.        , 0.        , 1.        , 1.        ]])\n\n\nAgain, it is very important to experiment with both of these techniques to see which one works for you."
  },
  {
    "objectID": "posts/2021-04-12-preprocess.html#conclusion",
    "href": "posts/2021-04-12-preprocess.html#conclusion",
    "title": "The essentials of data preprocessing for tabular data",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post we have seen the essentials of data preprocessing for tabular data. My goal with this blog post was to provide a condensed overview of the what data preprocessing looks like with useful code snippets that anyone can use in their projects. Applying proper preprocessing can be extremely helpful in improving the model performance and ensuring that your model is ready for the real world.\nIf hope you liked this blog post, please share it with other ML enthusiasts.\nSee you on the next adventure."
  },
  {
    "objectID": "posts/2022-03-26-lifeupdate.html",
    "href": "posts/2022-03-26-lifeupdate.html",
    "title": "Breaking Radio Silence - What Have I Been Doing in the Past Few Months",
    "section": "",
    "text": "Breaking Radio Silence - What Have I Been Doing in the Past Few Months\nHello everyone, I haven’t posted in a while, so I wanted to give you all some updates about where I have been.\nDon’t worry, everything is great, and I am as healthy as a horse 😊\nSo, what’s happening?\n\nLet’s start of with a good news. I recently became a Kaggle Discussions Expert.🥳 I have been posting on Kaggle discussions for over a year now and it has been a very fulfilling journey. One of the best ways to learn something is to first teach it to others. Answering questions from fellow kagglers has helped me in understanding new concepts and reinforcing my knowledge about the subject. I would definitely advice anyone starting out with data science to try answering queries on Kaggle Discussions and posting comments which can be helpful for others.\nI have been reading Abhishek Thakur’s book; Approaching Almost Any Machine Learning Problem. Filled with wonderful tricks and tips for machine learning practitioners, the book is a perfect read for anyone trying to understand how to approach a ML problem step-by-step. I personally loved the emphasis on trying out the code in the book and learning through practice.\nAs a final year student at VIT, I am currently working on my capstone project/bachelor’s thesis. In my university, all students must undertake a Capstone project in the final semester of their program to successfully complete their undergraduate degree. This is a great opportunity for students to work on a real world problem and put their skills to good use. The project is supervised by a faculty guide who aids and mentors students every step of the way. I have been long fascinated with the possibility of using NLP in the legal domain. So, I chose to work on contract review automation for my thesis. Contract review is the process of examining a contract comprehensively to understand the obligations and rights of a company signing it and evaluating its potential impact on the parties involved. With the project, our aim is to automate the task of contract reviews by using transformer models that can learn to automatically extract and identify key clauses from these contracts. This can be tremendously beneficial for law firms which spend a lot of their time reviewing contracts. Contract review costs also affect consumers. Since contract review costs are so prohibitive, contract review is not often performed outside corporate transactions. Small companies and individuals consequently often sign contracts without even reading them, which can result in predatory behavior that harms consumers. Read more about this from here: https://arxiv.org/abs/2103.06268\n\n\n\n\nImage from CUAD paper: https://arxiv.org/abs/2103.06268”\n\n\nLet me leave you with a verse from Madhushala by Harivansh Rai Bachchan:\n\nलाल सुरा की धार लपट सी कह न इसे देना ज्वाला, फेनिल मदिरा है, मत इसको कह देना उर का छाला, दर्द नशा है इस मदिरा का विगत स्मृतियाँ साकी हैं, पीड़ा में आनंद जिसे हो, आए मेरी मधुशाला।"
  },
  {
    "objectID": "posts/2021-05-03-pretrained.html",
    "href": "posts/2021-05-03-pretrained.html",
    "title": "Small Data And The Power Of Pre-Trained Models",
    "section": "",
    "text": "With the availability of better data storage facilities, more powerful computers and low latency networks, the commercial usage of huge datasets is rapidly growing in the field of machine learning. In this blog post, I would like to take a contrary position and advocate for the use of small datasets in making powerful deep learning applications."
  },
  {
    "objectID": "posts/2021-05-03-pretrained.html#up-until-now",
    "href": "posts/2021-05-03-pretrained.html#up-until-now",
    "title": "Small Data And The Power Of Pre-Trained Models",
    "section": "Up Until Now",
    "text": "Up Until Now\nIn 2008, Google processed 20 petabytes of data in a day. That was 13 years ago. The amount of data generated each day has been growing ever since. Which begs the question : How do we understand it and what do we do with it?\nBig Data Analytics is a branch of computer science focused at analyzing large amounts of data to uncover hidden patterns, correlations and other insights. Some common techniques in Big Data Analytics are data mining, text analytics, predictive analytics, data visualization, AI, machine learning, statistics and natural language processing. There is tremendous utility in analyzing massive amounts of data but there are some inherent difficulties in this domain:\n\nOnly the people with enough compute power and data storage capacity can produce competent work.\nIt requires extra investment for special data processing softwares and hardwares since the traditional ones cannot deal with the huge amount of data.\nTraining a machine learning model in this scenario takes a lot of time and resources.\n\nThe Big Data approach has led to a belief in the ML community that only large datasets can help you make meaningful ML projects. Although having sufficient good quality data is tremendously important, it is not necessarily true that you need large datasets. The bigger, the better stands to question now.\nWith advances in deep learning, we now have the capibility of using pre-trained models that need some customization and little amount of data to produce near perfect results. The technique of customizing a pre-trained model is called transfer learning. With transfer learning, the notions of implementing ML in the real world are being challenged and a new optimism about deep learning is rapidly spreading.\nLet’s see what transfer learning is and how it works."
  },
  {
    "objectID": "posts/2021-05-03-pretrained.html#small-data-and-transfer-learning",
    "href": "posts/2021-05-03-pretrained.html#small-data-and-transfer-learning",
    "title": "Small Data And The Power Of Pre-Trained Models",
    "section": "Small Data And Transfer Learning",
    "text": "Small Data And Transfer Learning\n\nWhat is transfer learning?\nTransfer learning, simply put, is the re-use of pre-trained models to solve a new task.\n\nIn transfer learning, a machine uses the knowledge gained from a previous task to solve a new task that it was not originally trained for. For example, a image classifier trained for recognizing modes of transport can be used to recognize different car models.\n\n\nHow does it work?\nA pre-trained model has millions of tuned parameters (weights and biases) that are used in performing the calculations which produce the final output predictions for a particular task. Transfer learning makes use of these tuned ‘numbers’ to solve a new task. These tuned ‘numbers’ represent knowledge about the real world that can be directly used in other scenarios where similar patterns occur in the data.\nThe final layers of a deep learning model determine the form of the output predictions we get. So, all we need to do is to ensure that the final layers of the pre-trained model are configured according to our new task.\nWe do this by chopping off the final layers of the pre-trained model and replacing them with customised new layers that are suitable for our new task. Then, we train the model using our dataset for a few epochs to fine tune it for the new task.\n &gt;Figure 1: This diagram shows how a pre-trained model is converted into a custom model by replacing the final layers.  Credits: Satya Mallick, https://learnopencv.com/\n\n\nSmall data\nAs opposed to Big Data, Small data is the data that consists of smaller datasets with a few thousand instances of labelled data. This is the kind of data that individuals have the resources to work with. This is the kind of data we find on Kaggle, UCI ML Repo and all over the internet. The Internet is crowded with small datasets that are considered ‘too small’ for making highly accurate deep learning models. But not for long, transfer learning allows us to use this abundance of small datasets to make production ready applications. In the next section, we will see a practical example of how transfer learning helps us to get great results on a small dataset."
  },
  {
    "objectID": "posts/2021-05-03-pretrained.html#a-practical-example",
    "href": "posts/2021-05-03-pretrained.html#a-practical-example",
    "title": "Small Data And The Power Of Pre-Trained Models",
    "section": "A Practical Example",
    "text": "A Practical Example\nIn this section, we are going to use transfer learning to create a model that can identify breeds of cats and dogs. The dataset used in this example has a total of 7393 images which is quite small for a computer vision dataset. The images represent 37 breeds of cats and dogs that are to be classified using our DL model.\nWe are using fastai to build our DL model and create our predictions. It is an easy to use deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches.\n\nGetting the data ready\nFirst things first, let’s download fastai.\n\n! [ -e /content ] && pip install -Uqq fastai  \n\n     |████████████████████████████████| 194kB 13.7MB/s \n     |████████████████████████████████| 12.8MB 24.4MB/s \n     |████████████████████████████████| 61kB 10.3MB/s \n     |████████████████████████████████| 776.8MB 24kB/s \nERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\n\n\nLet’s download the dataset. We are using the PETS dataset which is a standard computer vision dataset for multiclass classification.\n\nfrom fastai.vision.all import *\npath=untar_data(URLs.PETS)\n\n\n\n\nSetting the base path of the directory to easy access.\n\nPath.BASE_PATH=path\n\nWe have two folders in base directory: images and annotations. Here we will only use the images folder.\n\npath.ls()\n\n(#2) [Path('images'),Path('annotations')]\n\n\nWhat do we have in the annotations folder?\n\n(path/'annotations').ls()\n\n(#7) [Path('annotations/._trimaps'),Path('annotations/list.txt'),Path('annotations/README'),Path('annotations/xmls'),Path('annotations/test.txt'),Path('annotations/trainval.txt'),Path('annotations/trimaps')]\n\n\nWhat do we have in the images folder?\nAs you can see, there are 7393 images of dogs and cats. Each file name tells us what breed the pet in the image belongs to. Also, files starting with a capital letter represent the images of cats while files starting with a lower letter represent the images of dogs.\n\n(path/'images').ls()\n\n(#7393) [Path('images/pug_6.jpg'),Path('images/beagle_198.jpg'),Path('images/miniature_pinscher_163.jpg'),Path('images/beagle_69.jpg'),Path('images/beagle_120.jpg'),Path('images/Persian_52.jpg'),Path('images/Persian_131.jpg'),Path('images/Ragdoll_136.jpg'),Path('images/shiba_inu_4.jpg'),Path('images/english_setter_80.jpg')...]\n\n\nNow we have to preprocess our data and form mini-batches that can be directly fed into the model. Fastai gives us readymade functions that are super useful in getting the data ready.\n\npets=DataBlock(blocks=(ImageBlock,CategoryBlock),\n               get_items=get_image_files, #getting the images\n               splitter=RandomSplitter(seed=42), #splitting the data into training and validation set\n               get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'),'name'), #labelling the images\n               item_tfms=Resize(460),\n               batch_tfms=aug_transforms(size=224,min_scale=0.75)) #performing data augmentation \ndls=pets.dataloaders(path/\"images\")\n\nWe can also look at our data and see what the processed output looks like.\n\ndls.show_batch(nrows=2,ncols=4)\n\n\n\n\n\n\n\n\n\n\nTraining\nNow that our data is ready, we have to choose a learning rate that is going to give us the best results. To accomplish this, we will use the learning rate finder built-in in fastai. Our learning rate finder tells us which learning rate gives us the fastest decline in the loss function. From below, we can see that in our scenario a learning rate of around 1e-3 would be appropriate.\nWe are using the Resnet50 pre-trained model to create our customised pet breed identifier. Furthermore, we are going to track the progress of our model by using error rate as the metric.\n\nlearner=cnn_learner(dls,resnet50,metrics=error_rate) #we are using resnet50 as our pre-trained model with error rate as our metric\nlr_min,lr_steep=learner.lr_find()\n\n\n\n\n\n\n\n\n\n\n\nLet’s fit our pretrained model.\n\nimport time\nstart=time.time()\nlearner.fit_one_cycle(3,3e-3) #fine tuning the final layer\nlearner.unfreeze()\nlearner.fit_one_cycle(10,lr_max=slice(1e-5,1e-3)) #fine tuning the rest of the layers\nfin=time.time()\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.433581\n2.581600\n0.657645\n01:04\n\n\n1\n1.320106\n1.112681\n0.340325\n01:05\n\n\n2\n0.915266\n0.759420\n0.232070\n01:05\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.746069\n0.777516\n0.232747\n01:06\n\n\n1\n0.749254\n0.734976\n0.230717\n01:05\n\n\n2\n0.708324\n0.727369\n0.228011\n01:05\n\n\n3\n0.664956\n0.700559\n0.212449\n01:05\n\n\n4\n0.650105\n0.680652\n0.211773\n01:05\n\n\n5\n0.575446\n0.676955\n0.211773\n01:05\n\n\n6\n0.565760\n0.661803\n0.211096\n01:04\n\n\n7\n0.571792\n0.641877\n0.202300\n01:04\n\n\n8\n0.544800\n0.646405\n0.197564\n01:04\n\n\n9\n0.558373\n0.643165\n0.203654\n01:04\n\n\n\n\n\n\nHow much time did we train for?\n\nprint(f\"Model training time: {round((fin-start)/60)} minutes\")\n\nModel training time: 14 minutes\n\n\nWhat does the loss look like during training?\n\nlearner.recorder.plot_loss()\n\n\n\n\n\n\n\n\n\n\nModel Interpretation\nLet’s interpret the results of our model.\n\ninterpret=ClassificationInterpretation.from_learner(learner)\n\n\n\n\nWe can make a confusion matrix to interpret the results of our model.\n\ninterpret.plot_confusion_matrix(figsize=(12,12),dpi=60)\n\n\n\n\n\n\n\n\nWhich pet breeds are the most confusing for our model?\n\ninterpret.most_confused()[:5]\n\n[('Russian_Blue', 'British_Shorthair', 8),\n ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 8),\n ('english_cocker_spaniel', 'english_setter', 6),\n ('Birman', 'Ragdoll', 5),\n ('British_Shorthair', 'Russian_Blue', 5)]\n\n\n\n\nDoes it even work?\nBuilding the model and fitting it on our dataset is not sufficient. We need to check if our model really works. Let’s upload a random image and create the predictions. We will be using ipython widgets to create the upload button.\n\nimport ipywidgets as wig\n\nLet’s upload the image.\n\nupload= wig.FileUpload()\nupload\n\n\n\n\n\nimg = PILImage.create(upload.data[-1])\n\nWhat does our uploaded image look like?\n\ndisplay(img.to_thumb(256,256))\n\n\n\n\n\n\n\n\nLet’s create the predictions.\n\nprediction,pred_idx,probs=learner.predict(img)\nprint(\"Our prediction is:\",prediction)\n\n\n\n\nOur prediction is: Russian_Blue\n\n\nWith only 14 minutes of training we are able to make a highly accurate DL model that can classify 37 breeds of cats and dogs. What an amazing result!!!\nThis is not all. The task demonstrated here is a relatively simple one. With better pre-trained models and a little more training we can accomplish significant feats. The usage of these pre-trained models is not limited to the task they were originally designed for. For example, our Resnet50 model can not only be used for multiclass classification but also for image regression, multilabel classification and even image captioning. Sky is the limit!!!"
  },
  {
    "objectID": "posts/2021-05-03-pretrained.html#model-vendors-and-the-future",
    "href": "posts/2021-05-03-pretrained.html#model-vendors-and-the-future",
    "title": "Small Data And The Power Of Pre-Trained Models",
    "section": "Model Vendors And The Future",
    "text": "Model Vendors And The Future\nNow that we have seen the power of pre-trained models and transfer learning, let’s think about what the future might look like.\nWe will still need lots of people working with large datasets. All pre-trained models need to be trained by someone initially. Although training a model like Resnet50 takes a lot of data and time, it can be used by thousands of DL practitioners for making powerful applications afterwards. Moreover, we might see new jobs specifically catering to the growing need of pre-trained models in organizations. We might also see new enterprises that act as Model Vendors.\nModel Vendors will be organizations with the resources to develop models like Resnet, BERT, MobileNet, etc. They will sell these pre-trained models to DL practitioners who might use them in production after some customization. This is already happening to some extent with companies like OpenAI and Google who are releasing large pre-trained models (OpenAI’s GPT and Google’s T5). We might see more domain specific models that are not as large but are super useful for domain specialists.\nBut the most important thing is that we will no longer require large compute and storage resources to make DL applications that will serve the society and solve complex problems. All in all, the future looks optimistic for anybody with a curious mind and a working laptop."
  },
  {
    "objectID": "posts/2021-06-09-admissions.html",
    "href": "posts/2021-06-09-admissions.html",
    "title": "Understanding graduate admissions - Data driven advice for undergrad students",
    "section": "",
    "text": "What are the most important things in your masters application? How do you get into your dream institute? Is my GRE score enough? How much does CGPA matter?\nThese are questions that I have been obsessed with recently. If you are planning to go for Masters you know what I am talking about. In this blog post, I have tried my best to answer the above questions. But, there is a twist. I am going to use a data driven approach to answer the questions instead of citing online resources and personal anecdotes.\nSo let’s dive into it!"
  },
  {
    "objectID": "posts/2021-06-09-admissions.html#downloading-dataset-from-kaggle",
    "href": "posts/2021-06-09-admissions.html#downloading-dataset-from-kaggle",
    "title": "Understanding graduate admissions - Data driven advice for undergrad students",
    "section": "Downloading dataset from Kaggle",
    "text": "Downloading dataset from Kaggle\nThis blog post is built using Google Colab. It is a notebook server provided by Google for free. You can also use other services to run the code below but you will have to figure out how to get the dataset. The dataset that we use here is present on Kaggle and you can directly download it from here.\nIn this notebook, we are going to download the dataset from Kaggle into Google Colab and store it in a directory in our Google Drive. Storing your data in the Drive saves you from the trouble of downloading the dataset every time you start a new session in Colab.\nFor further guidance read this wonderful article by Mrinali Gupta: How to fetch Kaggle Datasets into Google Colab\nSo let’s get to it!\nFirst, we need to mount our google drive so that we can access all the folders in the drive.\n\n#collapse-output\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nThen we will using the following code to provide a config path for the Kaggle Json API\n\nimport os\nos.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/kaggle/GraduateAdmissions\"\n\nWe will change our current directory to where we want the dataset to be downloaded\n\n%cd /content/gdrive/My Drive/kaggle/GraduateAdmissions\n\n/content/gdrive/My Drive/kaggle/GraduateAdmissions\n\n\nNow we can download the dataset from kaggle\n\n!kaggle datasets download -d mohansacharya/graduate-admissions\n\nDownloading graduate-admissions.zip to /content/gdrive/My Drive/kaggle/GraduateAdmissions\n  0% 0.00/9.64k [00:00&lt;?, ?B/s]\n100% 9.64k/9.64k [00:00&lt;00:00, 1.33MB/s]\n\n\nLet’s unzip the files\n\n!unzip \\*.zip  && rm *.zip\n\nArchive:  graduate-admissions.zip\n  inflating: Admission_Predict.csv   \n  inflating: Admission_Predict_Ver1.1.csv  \n\n\nWhat files are present in the current directory?\n\n!ls\n\nAdmission_Predict.csv  Admission_Predict_Ver1.1.csv  kaggle.json\n\n\nOur directory has a file named ‘Admission_Predict_Ver1.1.csv’. That is our dataset. After we have downloaded our dataset, we have to install some important python libraries.\n\n#collapse-output\n! [ -e /content ] && pip install -Uqq fastai dtreeviz\n\nNow that our initial setup is done, let’s see what our data looks like."
  },
  {
    "objectID": "posts/2021-06-09-admissions.html#understanding-our-dataset",
    "href": "posts/2021-06-09-admissions.html#understanding-our-dataset",
    "title": "Understanding graduate admissions - Data driven advice for undergrad students",
    "section": "Understanding our dataset",
    "text": "Understanding our dataset\nBefore going forward, let’s import the essential python libraries.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random\nfrom fastai.tabular.all import *\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import r2_score,mean_squared_error,mean_squared_log_error\nfrom sklearn.inspection import plot_partial_dependence\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n\n\nOur dataset contains information about the profiles of 500 students who have applied to different colleges for their masters. Each row in the dataset contains information about the different parameters that are used by the universities to judge the merit of a candidate.\nLet’s take a look at our data.\n\ndf=pd.read_csv('Admission_Predict_Ver1.1.csv')\ndf.head(20)\n\n\n\n\n\n\n\n\n\nSerial No.\nGRE Score\nTOEFL Score\nUniversity Rating\n...\nLOR\nCGPA\nResearch\nChance of Admit\n\n\n\n\n0\n1\n337\n118\n4\n...\n4.5\n9.65\n1\n0.92\n\n\n1\n2\n324\n107\n4\n...\n4.5\n8.87\n1\n0.76\n\n\n2\n3\n316\n104\n3\n...\n3.5\n8.00\n1\n0.72\n\n\n3\n4\n322\n110\n3\n...\n2.5\n8.67\n1\n0.80\n\n\n4\n5\n314\n103\n2\n...\n3.0\n8.21\n0\n0.65\n\n\n5\n6\n330\n115\n5\n...\n3.0\n9.34\n1\n0.90\n\n\n6\n7\n321\n109\n3\n...\n4.0\n8.20\n1\n0.75\n\n\n7\n8\n308\n101\n2\n...\n4.0\n7.90\n0\n0.68\n\n\n8\n9\n302\n102\n1\n...\n1.5\n8.00\n0\n0.50\n\n\n9\n10\n323\n108\n3\n...\n3.0\n8.60\n0\n0.45\n\n\n10\n11\n325\n106\n3\n...\n4.0\n8.40\n1\n0.52\n\n\n11\n12\n327\n111\n4\n...\n4.5\n9.00\n1\n0.84\n\n\n12\n13\n328\n112\n4\n...\n4.5\n9.10\n1\n0.78\n\n\n13\n14\n307\n109\n3\n...\n3.0\n8.00\n1\n0.62\n\n\n14\n15\n311\n104\n3\n...\n2.0\n8.20\n1\n0.61\n\n\n15\n16\n314\n105\n3\n...\n2.5\n8.30\n0\n0.54\n\n\n16\n17\n317\n107\n3\n...\n3.0\n8.70\n0\n0.66\n\n\n17\n18\n319\n106\n3\n...\n3.0\n8.00\n1\n0.65\n\n\n18\n19\n318\n110\n3\n...\n3.0\n8.80\n0\n0.63\n\n\n19\n20\n303\n102\n3\n...\n3.0\n8.50\n0\n0.62\n\n\n\n\n20 rows × 9 columns\n\n\n\n\nSo, what are the parameters used by universities to judge the merit of a candidate? In this dataset, we have 6 important parameters that influence the selection of a candidate: 1. GRE Score - Your official GRE score\n2. TOEFL Score - Your official TOEFL score\n3. SOP (Statement of Purpose) - The rating of your SOP on a scale of 1-5\n4. LOR (Letter of Recommendation) - The rating of your LOR on a scale of 1-5\n5. CGPA - Your CGPA\n6. Research - Whether you have done some research or not (True or False)\nNow, obviously there are many more things that can influence your selection in a particular university. But, these are the bare essentials you need for applying to any graduate program. We also have a ‘University Rating’ column which represents how good a college is on the scale of 1 to 5. Another important column is the ‘Chance of Admit’, this column shows the probability of a student to get accepted in the institution he/she applied to.\nLet’s look at the kind of information each column contains.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 9 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Serial No.         500 non-null    int64  \n 1   GRE Score          500 non-null    int64  \n 2   TOEFL Score        500 non-null    int64  \n 3   University Rating  500 non-null    int64  \n 4   SOP                500 non-null    float64\n 5   LOR                500 non-null    float64\n 6   CGPA               500 non-null    float64\n 7   Research           500 non-null    int64  \n 8   Chance of Admit    500 non-null    float64\ndtypes: float64(4), int64(5)\nmemory usage: 35.3 KB\n\n\nAll the columns contain numeric information with int or float types. There are no columns in the dataset containing string or other types of information. We should also look at the number of unique values in each column to further understand our data.\n\ndf.nunique()\n\nSerial No.           500\nGRE Score             49\nTOEFL Score           29\nUniversity Rating      5\nSOP                    9\nLOR                    9\nCGPA                 184\nResearch               2\nChance of Admit       61\ndtype: int64\n\n\nFrom above, we can see that the ‘Research’ column contains only two values (0 and 1). This means that it is a boolean column. It does not provide us information about the kind of research done or the quantity of research done, it just tells us whether a student has done some research or not.\nWe can also see that the ‘Serial No.’ column can be dropped since it does not provide any useful information about a candidate’s profile.\n\ndf.drop(columns=['Serial No.'],inplace=True)\n\nWe have gathered some basic information about the data in our dataset. To understand our data, we need to carry out some Exploratory Data Analysis. This will help us in understanding what universities look for in a candidate, and what you must do to increase your chances of getting into your dream institute."
  },
  {
    "objectID": "posts/2021-06-09-admissions.html#exploratory-data-analysis",
    "href": "posts/2021-06-09-admissions.html#exploratory-data-analysis",
    "title": "Understanding graduate admissions - Data driven advice for undergrad students",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFirst, let’s plot scatter plots between the different parameters in our dataset and the ‘Chance of Admit’ column. This is a good starting point since it will let us see if their our any obvious relationships between our selected columns.\n\nscatter_plots=['GRE Score','TOEFL Score','SOP','LOR ','CGPA','Chance of Admit ']\nfig=plt.figure()\nfig.set_size_inches(15, 12)\nfig.subplots_adjust(hspace=0.25,wspace=0.25)\nfor i in range(len(scatter_plots)):\n  ax=fig.add_subplot(3,2,i+1)\n  sns.scatterplot(data=df, x=scatter_plots[i], y='Chance of Admit ') #adding kdeplots\nplt.show()\n\n\n\n\n\n\n\n\nFrom the scatter plots, we can see that there is linear relationship between the following variables: * GRE score and Chance of Admit * TOEFL score and Chance of Admit * CGPA and Chance of Admit\nThese relationships are fairly obvious, the more you score the better your chances of admission are. But, you can also see that there are outliers who have a low chance of getting admitted despite scoring high on these tests. This tells us that the other factors also play a vital role of assessing a candidate’s merit.\nFor parameters like SOP, LOR score, the relationship with ‘Chance of Admit’ looks sort of linear. Since these variables (SOP, LOR) have discrete levels, we can use a box plot to better capture their relationship with ‘Chance of Admit’.\nSo, let’s see a boxplot of SOP vs Chance of Admit.\n\nfig_dims = (10,5)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.boxplot(x='SOP',y='Chance of Admit ',data=df)\n\n\n\n\n\n\n\n\nThis looks much better. The above boxplot tells us that a good SOP significantly affects your chances of getting admitted into your dream college.\nNow, let’s look at the boxplot for LOR vs Chance of Admit.\n\nfig_dims = (10,5)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.boxplot(x='LOR ',y='Chance of Admit ',data=df)\n\n\n\n\n\n\n\n\nFrom the above boxplot, we can see how a good LOR increases your chances of getting admitted.\nFinally, we can also look at the correlations between the parameters in our dataset using a heatmap.\n\ncorrelation=df.corr()\nmask=np.triu(np.ones_like(correlation,dtype=np.bool))\nf,ax=plt.subplots(figsize=(15,10))\ncmap=sns.diverging_palette(220,10,as_cmap=True)\nsns.heatmap(correlation,mask=mask,cmap=cmap,vmax=1,vmin=-1,center=0,square=True,linewidths=.5,cbar_kws={'shrink':.5})\n\n\n\n\n\n\n\n\nThe above plots show that the better you perform on all these parameters, the higher your chances of getting admitted are. This can seem a very obvious thing to say, but it is important to notice that you need to be good at everything simultaneously. Just scoring high on the GRE or TOEFL won’t suffice. You have to ensure that your profile is great as a whole rather than having some parts that are excellent.\nModelling our data can be very helpful in understanding precisely how our chances of admission change. They can also help us in prioritizing things so that we can work in a focused manner.\nLet’s use some machine learning to better understand graduate admissions."
  },
  {
    "objectID": "posts/2021-06-09-admissions.html#data-preprocessing",
    "href": "posts/2021-06-09-admissions.html#data-preprocessing",
    "title": "Understanding graduate admissions - Data driven advice for undergrad students",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we use machine learning to interpret our data, we need to preprocess the data to make sure it is in the right form.\nThe first step is to split our data into a training set and a validation set.\n\ndset_size=df.shape[0] # dataset size\nvset_size=df.shape[0]*0.2 # validation set size\nvalid_idx= np.array(random.sample(range(dset_size),int(vset_size))) # validation set indices\ntrain_idx= list(np.setdiff1d(np.arange(dset_size),valid_idx)) # training set indices \nsplits=(train_idx,list(valid_idx))\ndset_size,vset_size\n\n(500, 100.0)\n\n\nThen we have to apply some data processing techniques to handle our data.\n\nprocs=[Categorify,FillMissing] # applying Label Encoding and Imputation\n\nWhile preprocessing our data, we have to define a dependent variable.The dependent variable is the column in our dataset whose values our ML model will predict. In this case, our dependent variable is the ‘Chance of Admit’ column.\n\ndep_var='Chance of Admit '\n\nWe also have to define which columns are continuous and which are categorical. Categorical columns contain data which has discrete levels whereas data in continuous columns can have any interger or float value.\n\ncont,cat= cont_cat_split(df,max_card=2, dep_var=dep_var)\n\nWhich are the categorical columns in our dataset?\n\ncat\n\n['Research']\n\n\nWhich are the continuous columns in our dataset?\n\ncont\n\n['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA']\n\n\nLet’s put it all together and preprocess our data.\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\nWhat does our data look like now?\n\nto.show(3)\n\n\n\n\n\n\nResearch\nGRE Score\nTOEFL Score\nUniversity Rating\nSOP\nLOR\nCGPA\nChance of Admit\n\n\n\n\n0\n1\n337\n118\n4\n4.5\n4.5\n9.65\n0.92\n\n\n1\n1\n324\n107\n4\n4.0\n4.5\n8.87\n0.76\n\n\n2\n1\n316\n104\n3\n3.0\n3.5\n8.00\n0.72"
  },
  {
    "objectID": "posts/2021-06-09-admissions.html#using-a-decision-tree",
    "href": "posts/2021-06-09-admissions.html#using-a-decision-tree",
    "title": "Understanding graduate admissions - Data driven advice for undergrad students",
    "section": "Using a Decision Tree",
    "text": "Using a Decision Tree\nNow that our data is ready, we will use a Decision Tree to predict the chances of admission. Decision Trees use a tree-like model of decisions with their corresponding consequences to predict the final outcome. They are very easy to interpret and often have great accuracy. As we will see, visualizing a decision tree can reveal some interesting insights.\nSo,let’s get the training and validation set.\n\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n\nLet’s can create a decision tree.\n\nm = DecisionTreeRegressor(max_leaf_nodes=20,min_samples_leaf=10,criterion='friedman_mse')\nm.fit(xs, y);\n\nBefore we visualize our decision tree, we have to ensure that it fits the data reasonably well. To measure how well our model fits the data, we are going to use R2-score as our metric. R2-score ranges from 0 to 1 with 1 being the best value.\nSo, what is the R2-score of our model?\n\nr2_score(valid_y,m.predict(valid_xs))\n\n0.7820742005320787\n\n\nThe R2-score looks good!\nNow, we will visualize the decision tree we have created.\n\ndtreeviz(m, xs, y, xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\n\n\n\n\n\n\nFrom the above visualization, we can see how our decision tree calculates the chance of admission of a student. Note that the tree will change if you use different hyperparameters for the model.\nAccording to our decision tree, CGPA is the most important feature for distinguishing the candidates. If you follow the diagram above, you can see that having a high CGPA is a significant advantage in your graduate admissions process.\nYou can also try finding the optimal hyperparameters for the above decision tree model. This will create a tree that is more specific and accurate."
  },
  {
    "objectID": "posts/2021-06-09-admissions.html#using-a-random-forest",
    "href": "posts/2021-06-09-admissions.html#using-a-random-forest",
    "title": "Understanding graduate admissions - Data driven advice for undergrad students",
    "section": "Using a Random Forest",
    "text": "Using a Random Forest\nRandom Forest is an ensemble learning approach where multiple decision trees are used to predict the final outcome. Each tree is trained on a subset of the data and the final prediction is calculated by averaging the predictions from all the trees. Random forests are very robust and do not overfit easily.\nLike decision trees, random forests can be interpreted using various techniques that lead to deep insights about the data.\nSo, let’s create a Random Forest.\n\ndef rf(xs, y, n_estimators=100,max_features='log2', min_samples_leaf=10, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,max_features=max_features,min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\nNow, let’s fit the model.\n\nm = rf(xs, y);\n\nAs in the previous section, we will R2-score to evaluate our model.\n\nr2_score(valid_y,m.predict(valid_xs))\n\n0.8157809694473572\n\n\nWe can also use other metrics (mean squared error, mean absolute error, etc) that tell us how much error our predictions have on average. Let’s take a look at the mean squared error for our model.\n\nmean_squared_error(valid_y,m.predict(valid_xs))\n\n0.0038503179006177434\n\n\nWhat about the mean squared log error?\n\nmean_squared_log_error(valid_y,m.predict(valid_xs))\n\n0.00155760558458151\n\n\nWhy does random forest work?\nThe working of a random forest model can seem rather counterintuitive. It is not obvious why using many decision trees instead of one leads to better outcomes. But, we can verify that it does lead to better outcomes and increases accuracy.\nWe can stack the predictions from all the models and see how the error changes as the number of estimators (decision trees) increases.\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\nLet’s calculate the mean squared error of the predictions while increasing the number of estimators in each iteration.\n\nerror_list=[mean_squared_error(preds[:i+1].mean(0), valid_y) for i in range(100)]\n\nWe can plot how the error changes with the number of decision trees.\n\nplt.plot(error_list);\n\n\n\n\n\n\n\n\nAs we increase the number of estimators, the mean squared error dramatically decreases. This means that ensembling a number of models increases accuracy and leads to better predictions.\nWe can interpret our model using a number of techniques. Here, we are going to use feature importance and partial dependence plots. Feature importance tells us how important each column is for making a prediction. We can get feature importances using an inbuilt attribute of random forest model. Let’s create a function to display feature importances in a proper format.\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n\nSo, what are our feature importances?\n\nfi = rf_feat_importance(m, xs)\nfi\n\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n6\nCGPA\n0.368737\n\n\n1\nGRE Score\n0.268672\n\n\n2\nTOEFL Score\n0.141553\n\n\n3\nUniversity Rating\n0.087964\n\n\n4\nSOP\n0.058644\n\n\n0\nResearch\n0.042200\n\n\n5\nLOR\n0.032229\n\n\n\n\n\n\n\n\nWe can plot our feature importances using a bar graph.\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\nplot_fi(fi);\n\n\n\n\n\n\n\n\nPartial Dependence Plot answers the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable? This brings out the relationship between the feature in question and the dependent variable.\nLet’s see the PDPs for the top 6 features in our dataset.\n\nstuff=list(fi.cols)\nstuff.remove('Research')\nfig,ax = plt.subplots(figsize=(15, 10))\nplot_partial_dependence(m, valid_xs,stuff ,grid_resolution=20, ax=ax);\n\n\n\n\n\n\n\n\nWhat can we infer from the above visualizations?\n\nThe feature importance plot tells us that the most important feature to predict the chances of admission of a candidate is the CGPA. Furthermore, the PDP for CGPA tells us that increasing the CGPA increases the chances of admission tremendously. This makes a lot of sense. If you have a high CGPA, you are presumably a great student.\nGRE and TOEFL score have second and third highest feature importances respectively. Again, we can see that the PDP for these features shows a similar trend as in the case of CGPA. A good score on these tests increases your chance of admission.\nSuprisingly, University Rating also affects the chance of admission. From the PDP, we see that increasing the value of University Rating increases the chance of admission. This does not make sense. It could be indicative of the bias in our data. Or, it could mean that the people who are applying to the higher ranking colleges are only applying there because they know they have a higher chance of getting admitted.\nFinally, higher ratings on the SOP and LOR also increase the chance of admission. Though, the increase is not as strong as for CGPA or GRE score, it still matters alot."
  },
  {
    "objectID": "posts/2021-06-09-admissions.html#what-should-you-do-to-get-into-a-high-ranking-university",
    "href": "posts/2021-06-09-admissions.html#what-should-you-do-to-get-into-a-high-ranking-university",
    "title": "Understanding graduate admissions - Data driven advice for undergrad students",
    "section": "What should you do to get into a high ranking university?",
    "text": "What should you do to get into a high ranking university?\nWe have seen different visualizations and techniques to understand our data and gather useful insights. In this section, I will try to answer the following question using the information from our dataset: What should you do to get into a high ranking university?\nFirst of all, let’s seperate the profiles of students who have applied to a high ranking university ( University Rating = 4 or 5 ) and have a high chance (&gt;80%) of getting into them.\n\ncond= ((df['University Rating']==4) | (df['University Rating']==5)) & (df['Chance of Admit ']&gt; .80)\nranking_idx=list(np.where(cond)[0])\n\nLet’s see how many profiles match our condition.\n\nhigh_rank_uni=df.iloc[ranking_idx]\nhigh_rank_uni\n\n\n\n\n\n\n\n\n\nGRE Score\nTOEFL Score\nUniversity Rating\nSOP\nLOR\nCGPA\nResearch\nChance of Admit\n\n\n\n\n0\n337\n118\n4\n4.5\n4.5\n9.65\n1\n0.92\n\n\n5\n330\n115\n5\n4.5\n3.0\n9.34\n1\n0.90\n\n\n11\n327\n111\n4\n4.0\n4.5\n9.00\n1\n0.84\n\n\n22\n328\n116\n5\n5.0\n5.0\n9.50\n1\n0.94\n\n\n23\n334\n119\n5\n5.0\n4.5\n9.70\n1\n0.95\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n482\n328\n113\n4\n4.0\n2.5\n8.77\n1\n0.83\n\n\n495\n332\n108\n5\n4.5\n4.0\n9.02\n1\n0.87\n\n\n496\n337\n117\n5\n5.0\n5.0\n9.87\n1\n0.96\n\n\n497\n330\n120\n5\n4.5\n5.0\n9.56\n1\n0.93\n\n\n499\n327\n113\n4\n4.5\n4.5\n9.04\n0\n0.84\n\n\n\n\n122 rows × 8 columns\n\n\n\n\nWhat do they have in common?\n\nfig=plt.figure()\nfig.set_size_inches(20, 20)\nfig.subplots_adjust(hspace=0.25,wspace=0.25)\ncols=list(df.columns)\nfor i in range(8):\n  ax=fig.add_subplot(4,2,i+1)\n  sns.kdeplot(high_rank_uni[cols[i]],shade=True) #adding kdeplots\nplt.show()\n\n\n\n\n\n\n\n\nHere’s my advice:\n\nIf you are in your second year of undergrad and still have time to save your pointer. Do it! If you are in your third year, try to boost it as much as you can. You should aim for a CGPA higher than 8.7. As I am saying this, my mind is telling me that this is an incredibly tough task. But if you accomplish it, it puts you in the group of people with high chances of getting admitted.\nA GRE score in the range 320 and above would be great. Most people with high chances of success fall in this range. This takes some effort and practice. But you come out with a better vocabulary and excellent reading,writing skills.\nGetting more than 110 on the TOEFL wouldn’t be a big deal for you. TOEFL is much easier than the GRE. So, you don’t need to worry about it much.\nDo some research. You need to do some research in the field you are interested in. This not only enhances your profile but prepares you for graduate school. Also, if you are not really sure about which field you want to pursue this is the perfect way to find out.\nFinally, use your awesome writing skills to create a SOP that tells the admission committee who you really are. You might want to look online before you do anything in this regard."
  },
  {
    "objectID": "posts/2021-06-09-admissions.html#conclusion",
    "href": "posts/2021-06-09-admissions.html#conclusion",
    "title": "Understanding graduate admissions - Data driven advice for undergrad students",
    "section": "Conclusion",
    "text": "Conclusion\nPlanning for graduate school is an exhausting task and requires a lot of thought. In this blog post, we have used data to understand how you can increase your chances of getting admitted into your dream institute.\nMy hope is that you take something valuable from this blog post and use it to accomplish your goals. If you found this helpful, please share it with your friends.\nSee you on the next adventure!!!"
  },
  {
    "objectID": "posts/2024-09-01-dsrag.html",
    "href": "posts/2024-09-01-dsrag.html",
    "title": "Unleashing Productivity - How In-House RAG Systems Can Turbocharge Your Data Science Team",
    "section": "",
    "text": "In today’s data-driven world, data science teams are the engines powering innovation, driving insights, and enabling informed decision-making across industries. As the complexity and volume of data continue to grow, so does the need for tools that streamline workflows and enhance productivity. Enter the in-house Retrieval-Augmented Generation (RAG) system, a pioneering solution designed to elevate the efficiency of data science teams. Unlike other RAG systems that rely on internet connectivity to fetch data, this system uses pre-downloaded information from key data science libraries, ensuring data privacy and security are maintained.\nIn this blog post, we’ll explore how an in-house RAG system can leverage documentation from popular data science libraries to boost productivity of data scientists. We’ll delve into its methodology, discuss the dataset it utilizes, examine its evaluation process, explore its user interface, and consider its potential impact on data science teams. Join us as we uncover how this innovative tool can revolutionize data science operations."
  },
  {
    "objectID": "posts/2024-09-01-dsrag.html#introduction",
    "href": "posts/2024-09-01-dsrag.html#introduction",
    "title": "Unleashing Productivity - How In-House RAG Systems Can Turbocharge Your Data Science Team",
    "section": "Introduction:",
    "text": "Introduction:\nImagine you’re a data scientist working on a crucial project with a tight deadline. You hit a roadblock: a burning question about how to efficiently aggregate data in Pandas. Naturally, you turn to ChatGPT or similar AI services to get quick answers and move forward. These tools have become indispensable for professionals seeking instant solutions to technical queries and coding challenges, helping streamline workflows and accelerate progress. However, while these AI services are immensely beneficial, they also come with significant challenges, particularly around data privacy, data relevance, and integration with proprietary tools. Furthermore, services like ChatGPT often fall short by not providing references alongside their answers. This lack of direct citations can hinder deeper research and verification of solutions, making it challenging to confirm the effectiveness of the advice given.\nEnter RAGs!!!\n\nWhat is a RAG?\nA Retrieval-Augmented Generation (RAG) system is a sophisticated AI tool that combines the capabilities of retrieving specific, relevant information from a predefined database with the generative power to provide coherent, contextually appropriate responses. Unlike standalone language models that depend solely on their training data, RAG systems augment their responses by accessing external sources, making them particularly valuable in technical fields like data science, where accuracy and timeliness of information are crucial. As companies seek more secure and customized solutions, RAG systems, such as those offered by platforms like You.com, are increasingly being recognized as viable alternatives to traditional AI services like ChatGPT.\n\n\n\nRAG Flowchart\n\n\n\n\nThe Necessity for an In-House RAG System:\nIn addition to the absence of citation capabilities, privacy concerns are paramount when using services like ChatGPT. Companies are naturally cautious about sending sensitive data to external servers, such as those managed by OpenAI, due to the risk of data exposure and leakage. This creates a substantial barrier for organizations with strict data privacy policies.\nMoreover, there are other challenges associated with using such services for technical queries:\n\nData Freshness and Relevance: These services may not always provide the most up-to-date information, as they rely on static datasets that can overlook the latest updates in technical documentation and library features.\nComplex Query Handling: While effective for general inquiries, these tools can struggle with complex or highly specific technical queries, often producing generic responses that may not fully address the user’s needs.\nIntegration with Proprietary Tools: Public AI services cannot accommodate the unique requirements of proprietary data science packages, limiting their effectiveness in customized workflows.\n\n\n\nGolden Retriever:\nIn this blog post, we will explore the importance and various aspects of an in-house Retrieval-Augmented Generation (RAG) system through the implementation of the Golden Retriever project. Built to meet the unique needs of data science teams, Golden Retriever works primarily on a local server, meaning it doesn’t need internet access to function. This ensures strong data privacy. The system uses pre-downloaded data from essential libraries to deliver accurate, personalized, and reference-rich responses. With its easy-to-use Streamlit interface and modular design, Golden Retriever is adaptable and scalable, making it an effective tool for maintaining up-to-date information and integrating proprietary tools."
  },
  {
    "objectID": "posts/2024-09-01-dsrag.html#methodology",
    "href": "posts/2024-09-01-dsrag.html#methodology",
    "title": "Unleashing Productivity - How In-House RAG Systems Can Turbocharge Your Data Science Team",
    "section": "Methodology:",
    "text": "Methodology:\nOur in-house Retrieval-Augmented Generation (RAG) system employs a sophisticated methodology to deliver accurate and context-rich responses for data science teams. Here’s a brief overview of our approach:\n\nDocument Preprocessing: We extract and clean data from popular data science libraries like Pandas,NumPy, and Scikit-learn using tools such as BSHTMLLoader and BeautifulSoup. This refined content is then stored as pickle files for efficient processing.\nEmbedding and Vector Storage: The preprocessed documents are chunked and embedded using the gte-large-en-v1.5 model. These embeddings are stored in ChromaDB vector databases, enabling rapid and accurate information retrieval.\nAdvanced Query Processing: Our system utilizes intelligent query processing techniques to understand complex technical queries and retrieve relevant documents.\nOutput Generation: We leverage the powerful llama-3.1-70b generative model to produce high-quality, tailored responses. The system combines retrieved documents with a prompt to generate comprehensive answers while also providing relevant document links as references.\nOffline Functionality: Designed to operate without internet connectivity, our RAG system is ideal for secure or isolated environments, ensuring data privacy and accessibility.\n\n\nDataset and Document Preprocessing:\nThis project utilizes a comprehensive dataset derived from the documentation of three widely-used data science libraries: Pandas, Scikit-learn, and NumPy. To create this dataset, we employ a fairly straightforward data acquisition process. Using the Python requests module, we download all the zipped documentation files for each library from their respective official sources. These compressed files are then systematically extracted, and the relevant text content is saved into a dedicated documents folder. This approach ensures that the RAG system has access to the most up-to-date and accurate information from these essential libraries. By incorporating documentation from these diverse sources, the system can provide comprehensive and context-rich responses to a wide range of data science queries, all while operating in an offline environment.\nThe document preprocessing stage is a crucial step in our RAG system’s pipeline. After downloading the documentation files, we employ a sophisticated approach to extract and refine the content of the input files. Using the BSHTMLLoader from Langchain, we parse the HTML files from the documentation. This loader efficiently extracts the relevant text content while preserving the structure. To further clean and optimize the extracted data, we utilize BeautifulSoup, which helps remove unnecessary HTML tags and formatting. This dual-layer approach ensures comprehensive content coverage while enabling detailed indexing. Finally, we store the preprocessed documents as pickle files, which allows for efficient storage and quick retrieval in subsequent stages of the RAG pipeline.\nThis meticulous preprocessing ensures that our system works with clean, well-structured data, ultimately enhancing the quality and relevance of the generated responses.\n\n\nVector Embeddings and Document Retrieval:\nThe next step in our RAG pipeline is to transform the preprocessed documents into searchable vector representations. We begin by loading the previously saved pickle files, which contain cleaned and structured content from popular data science libraries.\nFor generating embeddings, we leverage the gte-large-en-v1.5 model from Hugging Face, chosen for its superior ability to capture semantic relationships within technical documentation. The loaded documents undergo further processing using RecursiveCharacterTextSplitter, creating a dual-layer structure of parent documents (2000 characters with 200-character overlap) and child documents (500 characters with 50-character overlap). This approach ensures extensive content coverage while enabling detailed indexing.The RecursiveCharacterTextSplitter is particularly advantageous for our RAG system as it intelligently splits text based on character count while respecting the inherent structure of the document. It recursively splits on a list of characters (like “”, “”, ” “,”“) which helps maintain the semantic coherence of the content.\nTo facilitate efficient similarity searching, we implement ParentDocumentRetriever. It enables us to maintain broader context through parent documents while allowing for granular searches in child documents. This takes advantage of the hierarchical structure of our chunked documents, allowing for rapid and accurate retrieval of relevant information. The generated embeddings are then stored in ChromaDB vector database, enabling fast retrieval during query processing.\nThis carefully designed embedding and retrieval process forms the backbone of our RAG system. By transforming our extensive documentation into a searchable vector space, we ensure quick access to contextually relevant information that the LLM uses to generate an answer for the user query.\n\n\nPrompting and Output Generation:\nThe prompting and output generation process in our RAG system is a sophisticated orchestration of several key components. At the heart of this process is a custom memory system that stores conversation history using a simple yet effective list structure. This way the system is able to maintain context across multiple interactions without relying on external databases, ensuring seamless and coherent conversations even in offline environments.\nThe system’s document retrieval process is powered by the ParentDocumentRetriever, which performs a similarity search to fetch the top 4 most relevant documents. After searching for the relevant information, the system creates a customized prompt which is fed to the LLM.\nA carefully crafted prompt template serves as the blueprint for response generation. This template ingeniously combines the user’s query, conversation history, and retrieved documents, providing a comprehensive context to the language model. By structuring the input in this way, we guide the model to produce responses that are not only accurate but also contextually appropriate and coherent with previous interactions.\nFor the actual response generation, we leverage the power of the Llama 3.1 70B model through the Groq API. This API allows us to harness state-of-the-art language models while benefiting from cloud-based processing speed, crucial for maintaining system responsiveness. The generated output then undergoes custom post-processing to include relevant document links which the users can reference.\nThe system’s conversational loop ties all these elements together, enabling multi-turn interactions that maintain context throughout the conversation. This feature is particularly valuable for data scientists dealing with complex queries that often require follow-up questions or clarifications.\n\n\nUser Interface:\nThe Golden Retriever RAG system features a user-friendly interface built using Streamlit, providing an intuitive way for users to interact with the powerful RAG capabilities. This interface seamlessly integrates various components of the system, offering a clean, chat-like experience where users can input queries and receive detailed responses.\nThe Streamlit app first loads all the pre-processed documents and embeddings, setting up the RAG pipeline including the ParentDocumentRetriever and language model. As users interact with the system, their queries are processed through this pipeline, retrieving relevant documents and generating responses using the Llama 3.1 70B model via the Groq API. The interface displays these results along with reference links, ensuring traceability to source documentation.\nA key feature of the interface is its ability to manage conversation history, allowing for coherent multi-turn interactions. By presenting sophisticated RAG technology through a simple, accessible interface, the Golden Retriever system enhances workflow efficiency for data scientists.\n\n\n\nGolden Retriever Streamlit User Interface"
  },
  {
    "objectID": "posts/2024-09-01-dsrag.html#conclusion",
    "href": "posts/2024-09-01-dsrag.html#conclusion",
    "title": "Unleashing Productivity - How In-House RAG Systems Can Turbocharge Your Data Science Team",
    "section": "Conclusion:",
    "text": "Conclusion:\nImagine Sarah, a data scientist who frequently finds herself scouring Pandas documentation to refresh her memory on data frame aggregation techniques. With the Golden Retriever RAG system at her fingertips, Sarah can quickly access accurate, context-aware information without leaving her secure work environment. Instead of spending precious time navigating through multiple web pages, she can simply ask the system, “How do I aggregate data frames by multiple columns in Pandas?” and receive a concise, tailored response with relevant code snippets and documentation links.\nNow consider Alex, a junior data scientist tasked with building his first sentiment analysis system using the company’s proprietary NLP package. As he delves into this complex project, he encounters a myriad of questions: “What’s the best way to pre-process text data for sentiment analysis?” “How do I handle imbalanced classes in my sentiment dataset?” “Which machine learning models are most effective for sentiment classification?” With each query, the Golden Retriever system provides Alex with comprehensive answers, drawing from its vast knowledge base of Scikit-learn, NLTK, and the in-house libraries. The system not only offers explanations but also suggests best practices, potential pitfalls to avoid, and even code examples to jumpstart his implementation.\nBy leveraging the RAG system, both Sarah and Alex can work more efficiently, spending less time searching for information and more time applying their expertise to solve complex data science challenges.\nIn conclusion, our in-house Retrieval-Augmented Generation (RAG) system, Golden Retriever, represents a significant leap forward in empowering data science teams with efficient, accurate, and context-aware information retrieval. By leveraging advanced technologies such as state-of-the-art embedding models, intelligent document preprocessing, and powerful language models, we’ve created a system that operates seamlessly in offline environments while providing high-quality responses.\nCheck out the full project here."
  },
  {
    "objectID": "posts/2024-09-01-dsrag.html#references",
    "href": "posts/2024-09-01-dsrag.html#references",
    "title": "Unleashing Productivity - How In-House RAG Systems Can Turbocharge Your Data Science Team",
    "section": "References:",
    "text": "References:\n\nGithub Repo\nLangchain Website\nGroq API Documentation\nRecursive Character Splitting\nChroma DB\nModel Comparison with MTEB leaderboard\nLLaMA: Open and Efficient Foundation Language Models"
  },
  {
    "objectID": "posts/2021-08-14-imageregression.html",
    "href": "posts/2021-08-14-imageregression.html",
    "title": "Cat Face Mapper - Image Regression with FastAI",
    "section": "",
    "text": "Image regression is the approach of learning from a dataset where the independent variable is an image and the dependent variable is one or more floats. In this blog post, we will learn how to create a key-point model with FastAI. A key-point model is a variant of image regression. It is used to predict the location of a specific point(s) in an image.\nTo create our key-point model, we will be using the CAT dataset from Kaggle. It contains over 9,000 cat images. For each image, there are annotations for nine points on the face of a cat, two for eyes, one for mouth, and six for ears. Our task is to predict the locations of these points with our key-point model."
  },
  {
    "objectID": "posts/2021-08-14-imageregression.html#downloading-the-dataset",
    "href": "posts/2021-08-14-imageregression.html#downloading-the-dataset",
    "title": "Cat Face Mapper - Image Regression with FastAI",
    "section": "Downloading the dataset",
    "text": "Downloading the dataset\nThe first task in our pipeline is to download our dataset from Kaggle. In my previous blog posts, I have explained how to download a Kaggle dataset directly into the Google Drive. So, be sure to check those out if you want more explanation about what is going on here. Or, you can just copy the code shown here and use it in your work. Also, all the code shown here is implemented in Google Colab but you can use the notebook server of your choice. OK!!! Let’s start.\nMounting the google drive\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nWhere should the dataset be downloaded?\n\nimport os\nos.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/kaggle/Cats\" \n%cd /content/gdrive/My Drive/kaggle/Cats\n\n/content/gdrive/My Drive/kaggle/Cats\n\n\nDownloading the dataset from Kaggle\n\n!kaggle datasets download -d crawford/cat-dataset\n\nDownloading cat-dataset.zip to /content/gdrive/My Drive/kaggle/Cats\n100% 4.03G/4.04G [01:20&lt;00:00, 85.1MB/s]\n100% 4.04G/4.04G [01:20&lt;00:00, 53.6MB/s]\n\n\nUnzipping the files\n\n!unzip \\*.zip  && rm *.zip\n\nWhat files are present in the current directory?\n\n!ls\n\nCAT_00  CAT_01  CAT_02  CAT_03  CAT_04  CAT_05  CAT_06  cats  kaggle.json"
  },
  {
    "objectID": "posts/2021-08-14-imageregression.html#data-preprocessing-and-datablock",
    "href": "posts/2021-08-14-imageregression.html#data-preprocessing-and-datablock",
    "title": "Cat Face Mapper - Image Regression with FastAI",
    "section": "Data preprocessing and DataBlock",
    "text": "Data preprocessing and DataBlock\nOnce we have downloaded the dataset, we can work on our data preprocessing pipeline. In FastAI, preprocessing is carried out using the DataBlock API. To use it, we create a DataBlock object and pass in some details about how the data should be pre-processed. And then, it basically takes care of all the preprocessing steps for us.\nBut first, let’s download the latest version of FastAI.\n\n! [ -e /content ] && pip install -Uqq fastai\n\n     |████████████████████████████████| 188 kB 8.3 MB/s \n     |████████████████████████████████| 56 kB 4.9 MB/s \n\n\nNow let’s import the required modules.\n\nfrom fastai.vision.all import *\nfrom pathlib import Path\nimport numpy as np\nimport os\n\nBefore building a DataBlock, we will explore how our data is organized. In the previous section, we saw that there is a folder named ‘cats’ in the directory where we downloaded the dataset. This folder contains the images we are going to use.\n\npath=Path(os.getcwd()+'/cats')\npath\n\nPath('/content/gdrive/My Drive/kaggle/Cats/cats')\n\n\nFor the sake of convenience, we can store the path where our dataset is located in the BASE_PATH attribute of the Path class.\n\nPath.BASE_PATH=path\nPath.BASE_PATH\n\nPath('.')\n\n\nNow, let’s find out what’s inside the ‘cats’ folder.\n\npath.ls().sorted()\n\n(#7) [Path('CAT_00'),Path('CAT_01'),Path('CAT_02'),Path('CAT_03'),Path('CAT_04'),Path('CAT_05'),Path('CAT_06')]\n\n\nThe ‘cats’ folder contains sub-folders with names like ’CAT__00’, ’CAT__01’ , etc. But, what’s inside them ?\n\n(path/'CAT_00').ls().sorted()\n\n(#3413) [Path('CAT_00/00000001_000.jpg'),Path('CAT_00/00000001_000.jpg.cat'),Path('CAT_00/00000001_005.jpg'),Path('CAT_00/00000001_005.jpg.cat'),Path('CAT_00/00000001_008.jpg'),Path('CAT_00/00000001_008.jpg.cat'),Path('CAT_00/00000001_011.jpg'),Path('CAT_00/00000001_011.jpg.cat'),Path('CAT_00/00000001_012.jpg'),Path('CAT_00/00000001_012.jpg.cat')...]\n\n\nEach of these sub-folders contains different images of cats along with ‘.cat’ files which have the co-ordinates of face points for a their corresponding image.\nWe need to create a function that can return the ‘.cat’ file for a given image. We can accomplish this by simply adding ‘.cat’ to the end of a jpg file.\n\nimg_files=get_image_files(path)\ndef img2face(x): #returns the co-ordinates file for a given image\n  return Path(f'{str(x)[:-4]}.jpg.cat')\nimg2face(img_files[1]) #testing the function on an image from the dataset\n\nPath('CAT_00/00000390_001.jpg.cat')\n\n\nNow, let’s take a look at an image from our dataset.\n\nim=PILImage.create(img_files[0])\nim,shape #what is the shape of our image\n\n(PILImage mode=RGB size=500x279, None)\n\n\n\nim.to_thumb(250)\n\n\n\n\n\n\n\n\nWe still need to find a way to read the co-ordinates from the ‘.cat’ files. Luckily, Numpy provides us the genfromtxt function which can be used to load data from a text file. So, let’s try this function and see what we get.\n\nface=np.genfromtxt(path/'CAT_00'/'00000001_000.jpg.cat')[1:]\nface\n\narray([175., 160., 239., 162., 199., 199., 149., 121., 137.,  78., 166.,\n        93., 281., 101., 312.,  96., 296., 133.])\n\n\nWe can apply the img2face and genfromtxt functions to get the corresponding co-ordinates file for an image and get the face points from that file. We encapsulate all this functionality in another function which will be useful while building our DataBlock. Let’s call this function get_face_points.\n\ndef get_face_points(f):\n  face_points=np.genfromtxt(img2face(f))[1:]\n  return tensor(face_points)\n\nWe can test our get_face_points function by passing in the path of an image file. As you can see below, the function returns the co-ordinates present in the file.\n\nget_face_points(img_files[0])\n\ntensor([269., 132., 352., 134., 310., 187., 222.,  92., 210.,  24., 276.,  61.,\n        352.,  63., 417.,  26., 406.,  97.])\n\n\nLet’s mark these co-ordinates on the image and see if they actually represent the position of the facial features.\n\nfrom matplotlib import image\nfrom matplotlib import pyplot as plt\n\n# to read the image stored in the working directory\ndata = image.imread(img_files[0])\n\n#get the x,y coordinates\nt=get_face_points(img_files[0])\nxs=[t[i] for i in range(18) if i%2==0]\nys=[t[i] for i in range(18) if i%2!=0]\ncoords=[(float(i),float(j)) for i,j in zip(xs,ys)]\n\nfor i,j in coords:\n  plt.plot(i,j,marker='x',color='red')\nplt.imshow(data)\nplt.show()\n\n\n\n\n\n\n\n\nNice!!! Our get_face_points function returns the co-ordinates perfectly. Now, we will build our DataBlock. To create a DataBlock, we need to pass in the following details:\n\nType of independent and dependent variable (our independent variables are images and the dependent variables are points )\nA function to get the independent variables or images in our case (get_image_files) .\nA function to get the values of the dependent variable, which are the co-ordinates of facial features (get_face_points ).\nA method to split the dataset into training and validation set (FuncSplitter).\nFinally, item and batch transforms. Transforms allow us to manipulate our data. They are used to convert the images into the desired format and perform data augmentation. Here, we are using the Resize item transform which reshapes all the images into one shape (500,500).\n\n\ncats=DataBlock(\n    blocks=(ImageBlock,PointBlock),\n    get_items=get_image_files,\n    get_y=get_face_points,\n    splitter=FuncSplitter(lambda o: o.parent.name=='CAT_06'),\n    item_tfms=Resize(size=500,method='squish')\n)\n\nUsing our DataBlock, we create a dataloader. A dataloader divides our dataset into batches which can be fed into the model one at a time. Each batch is of the form: ((X1,X2,X3...),(Y1,Y2,Y3...)) where X1,X2,X3 are images and Y1,Y2,Y3 are co-ordinates corresponding to the images.\nLet’s look at a batch from our dataloader.\n\ndls=cats.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n\n\n\nWe can also look at the shape of the independent and dependent variables to understand how the data is organized in a batch.\n\nxb, yb= dls.one_batch()\nxb.shape,yb.shape\n\n(torch.Size([64, 3, 500, 500]), torch.Size([64, 9, 2]))\n\n\nEach batch in our dataloader has 64 RGB images of the size (500,500) and the corresponding x and y co-ordinates of the nine face points.\n\nyb[0] #the 2 in the final axis of the shape represents x and y co-ordinates \n\nTensorPoint([[-0.3080, -0.1650],\n        [ 0.0080, -0.2050],\n        [-0.1240,  0.1400],\n        [-0.5360, -0.2850],\n        [-0.6160, -0.7000],\n        [-0.3400, -0.5200],\n        [-0.0440, -0.5700],\n        [ 0.1800, -0.8450],\n        [ 0.1920, -0.3850]], device='cuda:0')"
  },
  {
    "objectID": "posts/2021-08-14-imageregression.html#resnet18-key-point-model",
    "href": "posts/2021-08-14-imageregression.html#resnet18-key-point-model",
    "title": "Cat Face Mapper - Image Regression with FastAI",
    "section": "ResNet18 Key-Point Model",
    "text": "ResNet18 Key-Point Model\nOur data is ready, let’s focus on creating our key-point model. We will use transfer learning and fine-tune a pretrained model instead of creating one from scratch. This is because training an accurate model from scratch will take a lot of time (hundreds of epochs) and compute power. On the other hand, transfer learning will help us in creating a model quickly, with less data while simultaneously ensuring top-notch results.\nFor our pretrained model, we will use the ResNet18 architecture trained on ImageNet. FastAI provides us with the cnn_learner convenience function to easily create and train a CNN model. All we need to do is pass in our dataloader, the architecture we want to use and the range of our targets. Our y_range (target range) is between -1 to +1. This is because coordinates in FastAI and PyTorch are always rescaled between -1 and +1.\n\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\n\n\n\n\n\n\nNotice that we did not mention a loss function in the cnn_learner but it still worked. This is because it uses the default loss function provided by FastAI. So, what is the default loss function for our key-point model?\n\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\nUsing an optimal learning rate is essential for ensuring that our model learns at the proper pace and finds the lowest possible value for the loss. The lr_find function in FastAI finds an optimal learning rate for us and returns a plot depicting the relationship between loss value and learning rate.\n\nlr=learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\nWhat is the suggested learning rate by lr_find?\n\nlr[0]\n\n0.0020892962347716093\n\n\nWith all the pieces in the right place, we can start the training process. To fine-tune our model, we need to pass in the number of epochs and the learning rate in the fine_tune function of our cnn_learner object. We will train our model for 20 epochs and see how accurate our model gets.\n\nlearn.fine_tune(20,lr[0])\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.346460\n0.123529\n32:17\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.171530\n0.090876\n04:57\n\n\n1\n0.125122\n0.069474\n04:53\n\n\n2\n0.081517\n0.044853\n04:53\n\n\n3\n0.050499\n0.036057\n04:55\n\n\n4\n0.034669\n0.021600\n04:56\n\n\n5\n0.025236\n0.023799\n04:56\n\n\n6\n0.021619\n0.020361\n04:56\n\n\n7\n0.018498\n0.014436\n04:56\n\n\n8\n0.015099\n0.012759\n04:55\n\n\n9\n0.013344\n0.012700\n04:55\n\n\n10\n0.011245\n0.009587\n04:55\n\n\n11\n0.009165\n0.008542\n04:55\n\n\n12\n0.008128\n0.007765\n04:54\n\n\n13\n0.007260\n0.007510\n04:54\n\n\n14\n0.006865\n0.007455\n04:54\n\n\n15\n0.006591\n0.007636\n04:55\n\n\n16\n0.006417\n0.007434\n04:57\n\n\n17\n0.005965\n0.007234\n04:57\n\n\n18\n0.005865\n0.007166\n04:57\n\n\n19\n0.005836\n0.007447\n04:56\n\n\n\n\n\n\nThat’s quite the result. We got to a loss of around 0.0074 with just 20 epochs. We can also plot the training and validation loss for the whole learning process.\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nNumbers are hard to understand, so let’s see the results and compare our predictions with the ground truth.\n\nlearn.show_results(ds_idx=1, nrows=4, figsize=(6,8))\n\n\n\n\n\n\n\n\n\n\n\nIndeed, our model has become quite good at this task.\nSince training our model took a while, it would be wise to save it in memory so we can just load it whenever required.\n\nlearn.export() #we can name the model also"
  },
  {
    "objectID": "posts/2021-08-14-imageregression.html#trying-a-deeper-architecture---resnet34",
    "href": "posts/2021-08-14-imageregression.html#trying-a-deeper-architecture---resnet34",
    "title": "Cat Face Mapper - Image Regression with FastAI",
    "section": "Trying a deeper architecture - ResNet34",
    "text": "Trying a deeper architecture - ResNet34\nIn the previous section, we used the ResNet18 architecture and got a loss of around 0.0074. But we can do better. We can use a deeper architecture. Deeper architectures usually model the data more accurately as compared to shallower ones. In general, a bigger model has the ability to better capture the real underlying relationships in your data, and also to capture and memorize the specific details of your individual images. But they require more compute power during training. Also, they take quite a bit longer to train.\nOne technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) wherever possible during training. This can speed up the training process by 2-3x. It also reduces memory usage during training. To enable this feature in FastAI, we just have to add to_fp16() after our cnn_learner creation .\nLet’s switch to ResNet34 to get better results in our task. To use ResNet34 ,we just need change the architecture we passed in the cnn_learner to resnet34. Rest all the details remain the same.\n\nlearn = cnn_learner(dls, resnet34, y_range=(-1,1)).to_fp16()\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\n\n\n\n\n\n\nWhat’s the loss function?\n\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\nUsing lr_find to get an optimal learning rate.\n\nlr=learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\nWhat is the suggested learning rate?\n\nlr[0]\n\n0.002511886414140463\n\n\nNow, let’s fine-tune our ResNet34 architecture for 20 epochs and see if there is any improvement.\n\nlearn.fine_tune(20,lr[0])\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.322153\n0.112187\n37:05\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.135973\n0.067399\n06:31\n\n\n1\n0.076629\n0.036782\n06:28\n\n\n2\n0.040184\n0.027252\n06:27\n\n\n3\n0.025203\n0.042123\n06:27\n\n\n4\n0.021205\n0.011077\n06:28\n\n\n5\n0.019296\n0.012121\n06:27\n\n\n6\n0.019582\n0.015632\n06:27\n\n\n7\n0.018242\n0.011337\n06:29\n\n\n8\n0.015452\n0.009075\n06:27\n\n\n9\n0.015463\n0.008312\n06:28\n\n\n10\n0.013015\n0.009817\n06:28\n\n\n11\n0.010838\n0.009142\n06:30\n\n\n12\n0.009916\n0.006813\n06:29\n\n\n13\n0.009414\n0.007926\n06:29\n\n\n14\n0.008769\n0.006682\n06:30\n\n\n15\n0.007947\n0.006979\n06:30\n\n\n16\n0.007133\n0.005844\n06:29\n\n\n17\n0.006951\n0.005805\n06:30\n\n\n18\n0.006743\n0.006596\n06:31\n\n\n19\n0.006953\n0.006431\n06:28\n\n\n\n\n\n\nAs you can see, ResNet34 manages to get a lower loss than the ResNet18. We might get a better loss value if we train this model for some more epochs. So go ahead and try it !!!\nLet’s look at the loss plot.\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nFinally, let’s look at the results from this model.\n\nlearn.show_results(ds_idx=1, nrows=4, figsize=(6,8))\n\n\n\n\n\n\n\n\n\n\n\nAgain, saving the model now will save you from a lot of trouble later.\n\nlearn.export('export2.pkl') #we can name the model also\n\nThe results from both our models are great. Till now, we have trained two highly accurate models that can predict the co-ordinates of facial features in the image of a cat. Next, let’s have some fun!!!"
  },
  {
    "objectID": "posts/2021-08-14-imageregression.html#cat-face-mapper-application",
    "href": "posts/2021-08-14-imageregression.html#cat-face-mapper-application",
    "title": "Cat Face Mapper - Image Regression with FastAI",
    "section": "Cat Face Mapper Application",
    "text": "Cat Face Mapper Application\nThe real test of any model’s effectiveness is to see if it works on random unseen data. In this section, we will create a small application to see if our models work on unseen images. We will create a GUI with ipywidgets where you can upload a photo of a cat and see the output produced by our models.\n\n\nCode\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n\nLet’s see if our saved models are present in the current directory.\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#2) [Path('export.pkl'),Path('export2.pkl')]\n\n\nOK, now let’s load the models.\n\nlearn_inf=load_learner(path/'export.pkl')\nlearn_inf2=load_learner(path/'export2.pkl')\n\nInitializing the widgets we are going to use in our GUI.\n\nbtn_upload=widgets.FileUpload() #upload button\nbtn_fpoints=widgets.Button(description='Get Face Points') #button that produces the final output \nlbl_orig = widgets.Label() #original image label \nout_pl=widgets.Output() #output original image\nmodel_select=widgets.Dropdown(\n    options=[('Resnet18',1),('Resnet34',2)],\n    value=1,\n    description=\"Select model:\") #dropdown to select the model Resnet18 or Resnet34 \n\nSince our models output co-ordinates, we can’t tell if they actually work unless we mark those co-ordinates on the image. So, we will create a function that marks the output co-ordinates on our input (uploaded) image.\n\ndef show_pred(img,coords):\n  w,h=img.size\n  for i,j in coords:\n    plt.plot((i/500)*w,(j/500)*h,marker='x',color='red')\n  plt.imshow(img)\n\nOur GUI has a Get Face Points button which will display the output predictions from the selected model when clicked. For this to happen, we need to create an event handler that does the following things when the Get Face Points button is clicked:\n1. Load the uploaded image. 2. Generate predictions on the uploaded image using the selected model (ResNet18 or ResNet34). 3. Mark the predictions (co-ordinates) on the input image. 4. Display the final output (image with co-ordinates marked on it ) .\n\ndef on_click_fpoints(change):\n  img=PILImage.create(btn_upload.data[-1])\n  lbl_orig.value=f'Uploaded Image:'\n  out_pl.clear_output()\n  with out_pl:display(img.to_thumb(256,256))\n  ind=model_select.index\n  if ind==0:\n    preds,_,_=learn_inf.predict(img)\n    print('Model: Resnet18 , Output Image:')\n  elif ind==1:\n    preds,_,_=learn_inf2.predict(img)\n    print('Model: Resnet34 , Output Image:')\n  show_pred(img,preds)\n\nbtn_fpoints.on_click(on_click_fpoints) #what to do when button is clicked \n\nLastly, we need to build a layout for our GUI.\n\nVBox([widgets.Label('Cat face mapper!'), \n      btn_upload,\n      model_select,\n      btn_fpoints,\n      lbl_orig,\n      out_pl])\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nModel: Resnet18 , Output Image:\nModel: Resnet34 , Output Image:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoila!!! Our Cat Face Mapper works."
  },
  {
    "objectID": "posts/2021-08-14-imageregression.html#conclusion",
    "href": "posts/2021-08-14-imageregression.html#conclusion",
    "title": "Cat Face Mapper - Image Regression with FastAI",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we learned how to perform image regression using FastAI. Specifically, we built a key-point model that predicts the co-ordinates of facial features in cat images. We used transfer learning for creating our image regression model with the pretrained model being ResNet18. We also experimented with ResNet34 and saw how deeper architectures can produce better results. Finally, we built a fun application to test our models on random unseen images.\nNote that the same technique can be used to create a model for detecting facial features in human photos. Be sure to experiment with the code shown here and apply it on other image regression datasets.\nSee you on the next adventure!!!"
  },
  {
    "objectID": "posts/2021-04-04-list-comprehensions.html",
    "href": "posts/2021-04-04-list-comprehensions.html",
    "title": "List Comprehensions - The elegant way of creating lists in python",
    "section": "",
    "text": "I have coded in other languages like C, C++ , java , R but none of them are as elegant as python. I mostly use python for data science and ML but there is nothing you cannot do with python. In this blog post we will explore an elegant and faster way of creating lists in python , list comprehensions.\nWhat are list comprehensions? List comprehensions provide a concise way to create lists .Common applications are to make new lists where each element is the result of some operations applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition. From python docs"
  },
  {
    "objectID": "posts/2021-04-04-list-comprehensions.html#why-should-you-use-list-comprehensions",
    "href": "posts/2021-04-04-list-comprehensions.html#why-should-you-use-list-comprehensions",
    "title": "List Comprehensions - The elegant way of creating lists in python",
    "section": "Why should you use list comprehensions?",
    "text": "Why should you use list comprehensions?\n1.They are easy to read and understand  As mentioned before , list comprehensions provide a concise way of creating lists . But what does that actually mean?Let’s look at an example.\n\nIn the example given below , we create a list of even numbers from 1 to 20 using a for loop and then a list comprehension .This is a pretty simple task and does not require a lot of code but it shows us how concise and elegant list comprehensions are.\nImplementation using a for loop\n\nl=[]\nfor x in range(1,20):\n  if x%2==0:\n    l.append(x)\nl\n\n[2, 4, 6, 8, 10, 12, 14, 16, 18]\n\n\nImplementation using a list comprehension\n\n[x for x in range(1,20) if x%2==0]\n\n[2, 4, 6, 8, 10, 12, 14, 16, 18]\n\n\nWow!!! We just needed to write one line of code to create a list that contains all the even numbers from 1 to 20 (20 not included) . Although , the above example performs a very simple task , we now know just how powerful list comprehensions are.  Even if we have to do something complex it is always better to use list comprehensions . Since all the logic is packed in one line of code , it is much easier to debug as compared to a for loop with lots of lines and lots of indentation.\n2.They are generally faster than for loops\nList comprehensions are gnerally faster than for loops. There are mainly two reasons :\n\nPython is written in C. List comprehensions do a much better job of moving the computation to the C level as compared to the slower computation done by python while we are using a for loop.\nWhile using the for loop , you have to use the append() function in each iteration to add an element to the list you want to create. Calling the append() function each time slows down the execution .\n\nBut why should you believe me? Test it out for yourself …  Python has an in-built module called time that provides us with easy to use functions to track the amount of time our program runs for.\nImporting the time library\n\nimport time\n\nIn this example, we will create a list that contains squares of even numbers and cubes of odd numbers. We will also calculate the amount of time each method takes to complete the computation.\nImplementation using for loop\n\nstart=time.time()\nl=[]\nfor i in range(0,20):\n  if i%2==0:\n    l.append(i**2)\n  else:\n    l.append(i**3)\nprint(l)\nprint(\"The execution time is:\",time.time()-start)\n\n[0, 1, 4, 27, 16, 125, 36, 343, 64, 729, 100, 1331, 144, 2197, 196, 3375, 256, 4913, 324, 6859]\nThe execution time is: 0.010234832763671875\n\n\nImplementation using a list comprehension\n\nstart=time.time()\nl=[x**2 if x % 2 == 0 else x**3 for x in range(0,20)]\nprint(l)\nprint(\"The execution time is:\",time.time()-start)\n\n[0, 1, 4, 27, 16, 125, 36, 343, 64, 729, 100, 1331, 144, 2197, 196, 3375, 256, 4913, 324, 6859]\nThe execution time is: 0.00024056434631347656\n\n\nWell as you can clearly see, list comprehensions are way faster . But maybe this is just dumb luck . Let’s find a better way to validate the speed of list comprehensions.\n\nThe best way to test our program is to use different values for our variables and run the program over and over . Earlier , we only calculated the squares and cubes of numbers in the range [1,20] . Now , we are going to use multiple ranges of numbers and see how our execution time changes . Then we wil compare the execution time of the for loop and the list comprehension.\nDefining the ranges\n\nranges=[[0,20],[0,50],[0,100],[0,500],[0,1000]]\n\nCreating a function that uses for loop to complete the given task\n\ndef for_loop(r):\n  start=time.time()\n  l=[]\n  for i in range(r[0],r[1]):\n    if i%2==0:\n      l.append(i**2)\n    else:\n      l.append(i**3)\n  print(\"The execution time is {} seconds\".format(time.time()-start))\n  return l \n\nHow fast is the for loop at completing the task\n\nfor r in ranges:\n  for_loop(r)\n\nThe execution time is 2.0742416381835938e-05 seconds\nThe execution time is 3.4332275390625e-05 seconds\nThe execution time is 5.817413330078125e-05 seconds\nThe execution time is 0.0003447532653808594 seconds\nThe execution time is 0.0009100437164306641 seconds\n\n\nCreating a function that uses list comprehension to complete the given task\n\ndef list_comprehension(r):\n  start=time.time()\n  l=[x**2 if x % 2 == 0 else x**3 for x in range(r[0],r[1])]\n  print(\"The execution time is {} seconds\".format(time.time()-start))\n  return l\n\nHow fast is the list comprehension at completing the task\n\nfor r in ranges:\n  list_comprehension(r)\n\nThe execution time is 1.52587890625e-05 seconds\nThe execution time is 3.337860107421875e-05 seconds\nThe execution time is 6.0558319091796875e-05 seconds\nThe execution time is 0.0002884864807128906 seconds\nThe execution time is 0.0005509853363037109 seconds\n\n\nComparing the execution times , we observe that list comprehensions are faster even when we increase the range of numbers .Hence proved , list comprehensions are faster than for loops.\n\nYou can mess around with the above programs by changing the ranges or the function bodies .It is always better to play around with the code and try to break it . \n\nFor the curious minds out there , here is another method you can use to validate whether list comps are faster than for loops\nDefining the ranges\n\nranges=[[0,20],[0,50],[0,100],[0,500],[0,1000]]\n\nCreating a function that uses for loop to complete the given task\n\ndef for_loop(r):\n  start=time.time()\n  l=[]\n  for i in range(r[0],r[1]):\n    if i%2==0:\n      l.append(i**2)\n    else:\n      l.append(i**3)\n  return l, time.time()-start\n\nCreating a function that uses list comprehension to complete the given task\n\ndef list_comprehension(r):\n  start=time.time()\n  l=[x**2 if x % 2 == 0 else x**3 for x in range(r[0],r[1])]\n  return l,time.time()-start\n\nLet’s see who is faster\n\nfor r in ranges:\n  _,t_for=for_loop(r)\n  _,t_list=list_comprehension(r)\n  if t_for&gt;t_list:\n    print(\"List comprehension was faster in case:\",r)\n  else:\n    print(\"For loop was faster in case:\",r)\n\nList comprehension was faster in case: [0, 20]\nList comprehension was faster in case: [0, 50]\nList comprehension was faster in case: [0, 100]\nList comprehension was faster in case: [0, 500]\nList comprehension was faster in case: [0, 1000]"
  },
  {
    "objectID": "posts/2021-04-04-list-comprehensions.html#what-else",
    "href": "posts/2021-04-04-list-comprehensions.html#what-else",
    "title": "List Comprehensions - The elegant way of creating lists in python",
    "section": "What else ?",
    "text": "What else ?\nIt’s not just about readability and speed . List comprehensions are awesome .\n\nLet’s see what else we can do with them.\n1.Nested ifs  We can use multiple if conditions in list comps. This increases the flexibility we have in terms of selecting elements from a range or a pre-defined list.It also makes it easier to manage multiple conditions.\nImplementation using a for loop\n\nl=[]\nfor y in range(0,100):\n  if y % 2 ==0:\n    if y % 5 == 0: \n      if y % 3 == 0:\n        l.append(y)\nl\n\n[0, 30, 60, 90]\n\n\nImplementation using a list comprehension\n\n[y for y in range(100) if y % 2 ==0 if y % 5 == 0 if y % 3==0 ]\n\n[0, 30, 60, 90]\n\n\n2.Working with strings  Often, we have to create lists from strings using some conditions or functions . Working with strings can become cumbersome if you are using a for loop , or worse , multiple for loops . Here , we will see how list comps can be used to find out the indices of all the vowels in a string.\nThe string\n\nword=\"pneumonoultramicroscopicsilicovolcanoconiosis\"\n\nCreating a list of vowels\n\nvowels=[\"a\",\"e\",\"i\",\"o\",\"u\"]\n\nImplementation using for loop\n\nl=[]\nfor idx,x in enumerate(word):\n  if x in vowels:\n    l.append({x:idx})\nl\n\n[{'e': 2},\n {'u': 3},\n {'o': 5},\n {'o': 7},\n {'u': 8},\n {'a': 12},\n {'i': 14},\n {'o': 17},\n {'o': 20},\n {'i': 22},\n {'i': 25},\n {'i': 27},\n {'o': 29},\n {'o': 31},\n {'a': 34},\n {'o': 36},\n {'o': 38},\n {'i': 40},\n {'o': 41},\n {'i': 43}]\n\n\nImplementation using list comprehension\n\n[{x:idx} for idx,x in enumerate(word) if x in vowels]\n\n[{'e': 2},\n {'u': 3},\n {'o': 5},\n {'o': 7},\n {'u': 8},\n {'a': 12},\n {'i': 14},\n {'o': 17},\n {'o': 20},\n {'i': 22},\n {'i': 25},\n {'i': 27},\n {'o': 29},\n {'o': 31},\n {'a': 34},\n {'o': 36},\n {'o': 38},\n {'i': 40},\n {'o': 41},\n {'i': 43}]\n\n\n3.Multiple for loops  The world is a simulation and the source code has multiple for loops. The ability to use multiple for loops to perform complex tasks is very important in the programming world . List comps allow us to use as many for loops as we want inside the brackets.\n\nl=[]\nfor i in [3,5,7,9]:\n  for j in [2,4,6,8]:\n    if i**j &gt; j**i:\n      l.append(i**j)\n    else:\n      l.append(j**i)\nl\n\n[9,\n 81,\n 729,\n 6561,\n 32,\n 1024,\n 15625,\n 390625,\n 128,\n 16384,\n 279936,\n 5764801,\n 512,\n 262144,\n 10077696,\n 134217728]\n\n\n\n[i**j if i**j &gt; j**i else j**i for i in [3,5,7,9] for j in [2,4,6,8]]\n\n[9,\n 81,\n 729,\n 6561,\n 32,\n 1024,\n 15625,\n 390625,\n 128,\n 16384,\n 279936,\n 5764801,\n 512,\n 262144,\n 10077696,\n 134217728]\n\n\n4.Nested list comprehensions  Often , we need to create a list that is a collection of multiple lists. Using nested list comps , we can again do this with one line of code . Suppose you have to create a program that prints multiplication tables, how would you do it ? Would you use for loops ? How many loops must you create? Also what form should the output be in?  Let’s see an easy way you could accomplish this task using list comps.\nImplementation using for loop\n\nl=[]\nfor x in range(11):\n  l1=[]\n  for y in range(11):\n    l1.append(x*y)\n  l.append(l1)\nl\n\n[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30],\n [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40],\n [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60],\n [0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70],\n [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80],\n [0, 9, 18, 27, 36, 45, 54, 63, 72, 81, 90],\n [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]]\n\n\nImplementation using list comprehension\n\n[[x*y for y in range(11)] for x in range(11)]\n\n[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30],\n [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40],\n [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60],\n [0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70],\n [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80],\n [0, 9, 18, 27, 36, 45, 54, 63, 72, 81, 90],\n [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]]"
  },
  {
    "objectID": "posts/2021-04-04-list-comprehensions.html#conclusion",
    "href": "posts/2021-04-04-list-comprehensions.html#conclusion",
    "title": "List Comprehensions - The elegant way of creating lists in python",
    "section": "Conclusion",
    "text": "Conclusion\nList comps are very versatile and you can use them in your projects to make your code faster , readable and modular. But more than that , list comps represent a better way of coding . A way of coding which involves getting rid of excessive clutter , making your programs concise and easier for people to understand.Getting rid of excessive clutter does not only mean making your program smaller, it means formulating your problem in the best possible way.\nAlbert Einstein used to say: &gt; If you can’t explain it to a six year old, you don’t understand it yourself.\nIf you can formulate your problem such that you can explain it with one line of code then you have a deep understanding about your problem . This will help you create better systems that not only perform complex tasks but are easier to understand in how they work .\n##A challenge for you\nOkay!!! This has been a fascinating journey . But every journey must end for a new journey to begin .  For your new journey to begin , here is a challenge. Given below is a program that can be used to create a list of numbers from the Fibonacci Series , your task is to implement this using list comprehension. It is even possible ? If not , why ? Or maybe there is a way ?\n\nSee you on the next adventure.\n\nn=int(input(\"Enter the number of digits you want:\"))\na=0 #first seed\nb=1 #second seed\nl=[1]\nfor _ in range(0,n-1):\n  sum=a+b \n  l.append(sum)\n  a=b\n  b=sum\nl\n\nEnter the number of digits you want:10\n\n\n[1, 1, 2, 3, 5, 8, 13, 21, 34, 55]"
  },
  {
    "objectID": "posts/2021-05-24-autoeda.html",
    "href": "posts/2021-05-24-autoeda.html",
    "title": "Exploring Auto-EDA in Python",
    "section": "",
    "text": "EDA (Exploratory Data Analysis) is the process of investigating and analyzing the data to summarize it’s main characteristics and gain a deep understanding of what it represents. Usually, we use data visualization techniques to create plots or visuals which are easier to understand and also help us in spotting patterns within the data. In this blog post, we are going to explore how EDA can be automated so that we can reduce the manual effort and time needed to understand our data.\nCurrently, there are many libraries in python that allow us to automate our EDA process, but here we are going to use the following three:\nWe will be using the HR Analytics dataset from Kaggle to experiment with Auto-EDA and see how useful it really is.\nFirst things first, let’s install our python libraries and download our data."
  },
  {
    "objectID": "posts/2021-05-24-autoeda.html#initial-setup",
    "href": "posts/2021-05-24-autoeda.html#initial-setup",
    "title": "Exploring Auto-EDA in Python",
    "section": "Initial Setup",
    "text": "Initial Setup\n\nInstalling the libraries\nFirst, we are going to install our python libraries. To see the install instructions of the libraries in detail, you can check out their Github repositories: - SweetViz - Pandas Profiling - AutoViz\nWe are going to use the ‘pip’ command as usual to install the libraries.\n\n#collapse-output\n!pip install sweetviz autoviz \n\nCollecting sweetviz\n  Downloading https://files.pythonhosted.org/packages/6c/ab/5ef84ee47e5344e9a23d69e0b52c11aba9d8be658d4e85c3c52d4e7d9130/sweetviz-2.1.0-py3-none-any.whl (15.1MB)\n     |████████████████████████████████| 15.1MB 308kB/s \nCollecting autoviz\n  Downloading https://files.pythonhosted.org/packages/89/20/8c8c64d5221cfcbc54679f4f048a08292a16dbad178af7c78541aa3af730/autoviz-0.0.81-py3-none-any.whl\nRequirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.19.5)\nCollecting tqdm&gt;=4.43.0\n  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n     |████████████████████████████████| 81kB 7.9MB/s \nRequirement already satisfied: scipy&gt;=1.3.2 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.4.1)\nRequirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (2.11.3)\nRequirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.1.5)\nRequirement already satisfied: matplotlib&gt;=3.1.3 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (3.2.2)\nRequirement already satisfied: importlib-resources&gt;=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (5.1.2)\nRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from autoviz) (5.5.0)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (from autoviz) (0.90)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from autoviz) (0.11.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autoviz) (0.22.2.post1)\nRequirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from autoviz) (1.0.0)\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from autoviz) (0.10.2)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2&gt;=2.11.1-&gt;sweetviz) (2.0.0)\nRequirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;sweetviz) (2018.9)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;sweetviz) (2.8.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.1.3-&gt;sweetviz) (2.4.7)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.1.3-&gt;sweetviz) (1.3.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.1.3-&gt;sweetviz) (0.10.0)\nRequirement already satisfied: zipp&gt;=0.4; python_version &lt; \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources&gt;=1.2.0-&gt;sweetviz) (3.4.1)\nRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (2.6.1)\nRequirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (0.8.1)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (0.7.5)\nRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (4.4.2)\nRequirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (56.1.0)\nRequirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (1.0.18)\nRequirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (5.0.5)\nRequirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (4.8.0)\nRequirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;autoviz) (1.0.1)\nRequirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (5.3.1)\nRequirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (7.6.3)\nRequirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (5.6.1)\nRequirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (4.10.1)\nRequirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (5.1.0)\nRequirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (5.2.0)\nRequirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels-&gt;autoviz) (0.5.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;sweetviz) (1.15.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython-&gt;autoviz) (0.2.5)\nRequirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets&gt;=4.2-&gt;ipython-&gt;autoviz) (0.2.0)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"-&gt;ipython-&gt;autoviz) (0.7.0)\nRequirement already satisfied: tornado&gt;=4 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (5.1.1)\nRequirement already satisfied: jupyter-core&gt;=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (4.7.1)\nRequirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (0.9.5)\nRequirement already satisfied: jupyter-client&gt;=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (5.3.5)\nRequirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (5.1.3)\nRequirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (1.5.0)\nRequirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;autoviz) (3.5.1)\nRequirement already satisfied: jupyterlab-widgets&gt;=1.0.0; python_version &gt;= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;autoviz) (1.0.0)\nRequirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (3.3.0)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (0.8.4)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (1.4.3)\nRequirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (0.3)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (0.7.1)\nRequirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (0.4.4)\nRequirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole-&gt;jupyter-&gt;autoviz) (1.9.0)\nRequirement already satisfied: pyzmq&gt;=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole-&gt;jupyter-&gt;autoviz) (22.0.3)\nRequirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat-&gt;notebook-&gt;jupyter-&gt;autoviz) (2.6.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;autoviz) (20.9)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;autoviz) (0.5.1)\nInstalling collected packages: tqdm, sweetviz, autoviz\n  Found existing installation: tqdm 4.41.1\n    Uninstalling tqdm-4.41.1:\n      Successfully uninstalled tqdm-4.41.1\nSuccessfully installed autoviz-0.0.81 sweetviz-2.1.0 tqdm-4.60.0\n\n\nPandas Profiling provides some specific instructions for installation so let’s follow those.\n\n#collapse-output\nimport sys\n!{sys.executable} -m pip install -U pandas-profiling[notebook]\n!jupyter nbextension enable --py widgetsnbextension\n\nCollecting pandas-profiling[notebook]\n  Downloading https://files.pythonhosted.org/packages/3b/a3/34519d16e5ebe69bad30c5526deea2c3912634ced7f9b5e6e0bb9dbbd567/pandas_profiling-3.0.0-py2.py3-none-any.whl (248kB)\n     |████████████████████████████████| 256kB 6.8MB/s \nCollecting PyYAML&gt;=5.0.0\n  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n     |████████████████████████████████| 645kB 9.4MB/s \nCollecting requests&gt;=2.24.0\n  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n     |████████████████████████████████| 61kB 6.1MB/s \nRequirement already satisfied, skipping upgrade: seaborn&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (0.11.1)\nRequirement already satisfied, skipping upgrade: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (1.1.5)\nCollecting htmlmin&gt;=0.1.12\n  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\nRequirement already satisfied, skipping upgrade: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (2.11.3)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (1.19.5)\nRequirement already satisfied, skipping upgrade: scipy&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (1.4.1)\nRequirement already satisfied, skipping upgrade: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (0.4.2)\nRequirement already satisfied, skipping upgrade: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (4.60.0)\nCollecting phik&gt;=0.11.1\n  Downloading https://files.pythonhosted.org/packages/b7/ce/193e8ddf62d4be643b9b4b20e8e9c63b2f6a20f92778c0410c629f89bdaa/phik-0.11.2.tar.gz (1.1MB)\n     |████████████████████████████████| 1.1MB 16.3MB/s \nRequirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (1.0.1)\nCollecting tangled-up-in-unicode==0.1.0\n  Downloading https://files.pythonhosted.org/packages/93/3e/cb354fb2097fcf2fd5b5a342b10ae2a6e9363ba435b64e3e00c414064bc7/tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1MB)\n     |████████████████████████████████| 3.1MB 29.5MB/s \nCollecting pydantic&gt;=1.8.1\n  Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n     |████████████████████████████████| 10.1MB 32.7MB/s \nCollecting visions[type_image_path]==0.7.1\n  Downloading https://files.pythonhosted.org/packages/80/96/01e4ba22cef96ae5035dbcf0451c2f4f859f8f17393b98406b23f0034279/visions-0.7.1-py3-none-any.whl (102kB)\n     |████████████████████████████████| 112kB 40.0MB/s \nRequirement already satisfied, skipping upgrade: matplotlib&gt;=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (3.2.2)\nRequirement already satisfied, skipping upgrade: ipywidgets&gt;=7.5.1; extra == \"notebook\" in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (7.6.3)\nRequirement already satisfied, skipping upgrade: jupyter-core&gt;=4.6.3; extra == \"notebook\" in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (4.7.1)\nCollecting jupyter-client&gt;=6.0.0; extra == \"notebook\"\n  Downloading https://files.pythonhosted.org/packages/77/e8/c3cf72a32a697256608d5fa96360c431adec6e1c6709ba7f13f99ff5ee04/jupyter_client-6.1.12-py3-none-any.whl (112kB)\n     |████████████████████████████████| 122kB 38.3MB/s \nRequirement already satisfied, skipping upgrade: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling[notebook]) (3.0.4)\nRequirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling[notebook]) (2.10)\nRequirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling[notebook]) (2020.12.5)\nRequirement already satisfied, skipping upgrade: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling[notebook]) (1.24.3)\nRequirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas-profiling[notebook]) (2018.9)\nRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas-profiling[notebook]) (2.8.1)\nRequirement already satisfied, skipping upgrade: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2&gt;=2.11.1-&gt;pandas-profiling[notebook]) (2.0.0)\nRequirement already satisfied, skipping upgrade: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&gt;=1.8.1-&gt;pandas-profiling[notebook]) (3.7.4.3)\nRequirement already satisfied, skipping upgrade: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (2.5.1)\nRequirement already satisfied, skipping upgrade: bottleneck in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (1.3.2)\nCollecting multimethod==1.4\n  Downloading https://files.pythonhosted.org/packages/7a/d0/ce5ad0392aa12645b7ad91a5983d6b625b704b021d9cd48c587630c1a9ac/multimethod-1.4-py2.py3-none-any.whl\nRequirement already satisfied, skipping upgrade: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (21.2.0)\nRequirement already satisfied, skipping upgrade: Pillow; extra == \"type_image_path\" in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (7.1.2)\nCollecting imagehash; extra == \"type_image_path\"\n  Downloading https://files.pythonhosted.org/packages/8e/18/9dbb772b5ef73a3069c66bb5bf29b9fb4dd57af0d5790c781c3f559bcca6/ImageHash-4.2.0-py2.py3-none-any.whl (295kB)\n     |████████████████████████████████| 296kB 34.7MB/s \nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling[notebook]) (2.4.7)\nRequirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling[notebook]) (1.3.1)\nRequirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling[notebook]) (0.10.0)\nRequirement already satisfied, skipping upgrade: jupyterlab-widgets&gt;=1.0.0; python_version &gt;= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (1.0.0)\nRequirement already satisfied, skipping upgrade: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (5.1.3)\nRequirement already satisfied, skipping upgrade: traitlets&gt;=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (5.0.5)\nRequirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (3.5.1)\nRequirement already satisfied, skipping upgrade: ipython&gt;=4.0.0; python_version &gt;= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (5.5.0)\nRequirement already satisfied, skipping upgrade: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (4.10.1)\nRequirement already satisfied, skipping upgrade: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client&gt;=6.0.0; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (22.0.3)\nRequirement already satisfied, skipping upgrade: tornado&gt;=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client&gt;=6.0.0; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (5.1.1)\nRequirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas-profiling[notebook]) (1.15.0)\nRequirement already satisfied, skipping upgrade: decorator&lt;5,&gt;=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx&gt;=2.4-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (4.4.2)\nRequirement already satisfied, skipping upgrade: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash; extra == \"type_image_path\"-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (1.1.1)\nRequirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.2.0)\nRequirement already satisfied, skipping upgrade: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (2.6.0)\nRequirement already satisfied, skipping upgrade: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (5.3.1)\nRequirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.7.5)\nRequirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (2.6.1)\nRequirement already satisfied, skipping upgrade: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.8.1)\nRequirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (4.8.0)\nRequirement already satisfied, skipping upgrade: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (1.0.18)\nRequirement already satisfied, skipping upgrade: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (56.1.0)\nRequirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (1.5.0)\nRequirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (5.6.1)\nRequirement already satisfied, skipping upgrade: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.9.5)\nRequirement already satisfied, skipping upgrade: ptyprocess&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"-&gt;ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.7.0)\nRequirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.2.5)\nRequirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (3.3.0)\nRequirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.7.1)\nRequirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.4.4)\nRequirement already satisfied, skipping upgrade: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.8.4)\nRequirement already satisfied, skipping upgrade: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (1.4.3)\nRequirement already satisfied, skipping upgrade: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.3)\nRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (20.9)\nRequirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == \"notebook\"-&gt;pandas-profiling[notebook]) (0.5.1)\nBuilding wheels for collected packages: htmlmin, phik\n  Building wheel for htmlmin (setup.py) ... done\n  Created wheel for htmlmin: filename=htmlmin-0.1.12-cp37-none-any.whl size=27085 sha256=2889c06fcf32178bcfcd894aa597f8a6f0651e0e53d1d8a3051fb1deba3f03b4\n  Stored in directory: /root/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n  Building wheel for phik (setup.py) ... done\n  Created wheel for phik: filename=phik-0.11.2-cp37-none-any.whl size=1107413 sha256=a2c7b4e43b5c0b9fe7c51601923c17c3f39f88ec55bb08e7490cf4a73bd8fdd5\n  Stored in directory: /root/.cache/pip/wheels/c0/a3/b0/f27b1cfe32ea131a3715169132ff6d85653789e80e966c3bf6\nSuccessfully built htmlmin phik\nERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\nERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\nERROR: phik 0.11.2 has requirement scipy&gt;=1.5.2, but you'll have scipy 1.4.1 which is incompatible.\nInstalling collected packages: PyYAML, requests, htmlmin, phik, tangled-up-in-unicode, pydantic, multimethod, imagehash, visions, jupyter-client, pandas-profiling\n  Found existing installation: PyYAML 3.13\n    Uninstalling PyYAML-3.13:\n      Successfully uninstalled PyYAML-3.13\n  Found existing installation: requests 2.23.0\n    Uninstalling requests-2.23.0:\n      Successfully uninstalled requests-2.23.0\n  Found existing installation: jupyter-client 5.3.5\n    Uninstalling jupyter-client-5.3.5:\n      Successfully uninstalled jupyter-client-5.3.5\n  Found existing installation: pandas-profiling 1.4.1\n    Uninstalling pandas-profiling-1.4.1:\n      Successfully uninstalled pandas-profiling-1.4.1\nSuccessfully installed PyYAML-5.4.1 htmlmin-0.1.12 imagehash-4.2.0 jupyter-client-6.1.12 multimethod-1.4 pandas-profiling-3.0.0 phik-0.11.2 pydantic-1.8.2 requests-2.25.1 tangled-up-in-unicode-0.1.0 visions-0.7.1\nEnabling notebook extension jupyter-js-widgets/extension...\n      - Validating: OK\n\n\n\n\nDownloading dataset from Kaggle\nThis blog post is built using Google Colab. It is a notebook server provided by Google for free. You can also use other services to run the code below but you will have to figure out how to get the dataset. The dataset that we use here is present on Kaggle and you can directly download it from here.\nIn this notebook, we are going to download the dataset from Kaggle into Google Colab and store it in a directory in our Google Drive. Storing your data in the Drive saves you from the trouble of downloading the dataset every time you start a new session in Colab.\nFor further guidance read this wonderful article by Mrinali Gupta: How to fetch Kaggle Datasets into Google Colab\nSo let’s get to it!\nFirst, we need to mount our google drive so that we can access all the folders in the drive.\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nThen we will using the following code to provide a config path for the Kaggle Json API\n\nimport os\nos.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/kaggle/Insurance\"\n\nWe will change our current directory to where we want the dataset to be downloaded\n\n%cd /content/gdrive/My Drive/kaggle/Insurance\n\n/content/gdrive/My Drive/kaggle/Insurance\n\n\nNow we can download the dataset from kaggle\n\n!kaggle datasets download -d giripujar/hr-analytics\n\nDownloading hr-analytics.zip to /content/gdrive/My Drive/kaggle/Insurance\n  0% 0.00/111k [00:00&lt;?, ?B/s]\n100% 111k/111k [00:00&lt;00:00, 15.9MB/s]\n\n\nLet’s unzip the files\n\n!unzip \\*.zip  && rm *.zip\n\nArchive:  hr-analytics.zip\n  inflating: HR_comma_sep.csv        \n\n\nWhat files are present in the current directory?\n\n!ls\n\nAutoViz_Plots  HR_comma_sep.csv  kaggle.json\n\n\nOur directory has a file named ‘HR_comma_sep.csv’. That is our dataset."
  },
  {
    "objectID": "posts/2021-05-24-autoeda.html#auto-eda",
    "href": "posts/2021-05-24-autoeda.html#auto-eda",
    "title": "Exploring Auto-EDA in Python",
    "section": "Auto-EDA",
    "text": "Auto-EDA\nWhat does our data look like?\n\nimport pandas as pd\ndf=pd.read_csv('HR_comma_sep.csv')\ndf.head(5)\n\n\n\n\n\n\n\n\n\nsatisfaction_level\nlast_evaluation\nnumber_project\naverage_montly_hours\ntime_spend_company\nWork_accident\nleft\npromotion_last_5years\nDepartment\nsalary\n\n\n\n\n0\n0.38\n0.53\n2\n157\n3\n0\n1\n0\nsales\nlow\n\n\n1\n0.80\n0.86\n5\n262\n6\n0\n1\n0\nsales\nmedium\n\n\n2\n0.11\n0.88\n7\n272\n4\n0\n1\n0\nsales\nmedium\n\n\n3\n0.72\n0.87\n5\n223\n5\n0\n1\n0\nsales\nlow\n\n\n4\n0.37\n0.52\n2\n159\n3\n0\n1\n0\nsales\nlow\n\n\n\n\n\n\n\n\nFrom above, we can see that we have a tabular dataset with 10 columns and a target feature (‘left’). We have 4 categorical features (‘Department’,‘salary’,‘Work_accident’,‘promotion_last_5years’) apart from the target and 5 numerical features.\nThe goal of EDA is to gain insights which are not apparent just by looking at the data. To do this, we use different kinds of visualizations and statistical techniques which transform our data into information that we can interpret and analyze. With these insights, we can decide what to do with our data, how to process it, how to model it and how to present it in an understanble form.\nIn this practical example, we are going to use our Auto-EDA tools and note down the insights gained about the data using each one. This well help us in examining their utility and limitations.\n\nSweetViz\nSweetViz allows us to create a report which contains a detailed analysis of the data in each column along with information about the associations between the features. Let’s use SweetViz to create a report and see what insights we can gather from it.\n\nimport sweetviz as sv\nmy_report = sv.analyze(df,target_feat='left')\nmy_report.show_notebook(w='100%')\n\n\n\n\n \n\n\nInsights:\n\nThere are no missing values in any of the columns. This is an important insight, since it tells us that we do not need to perform any imputation during our preprocessing steps. It is also important for deciding which columns we want to keep while creating a ML model since missing data can have a signficant impact on the model’s performance.\nFor each numerical column, we can see various summary statistics like range, standard deviation, median, average and skewness, etc. This allows us to get a sense the data in these columns. This may be very helpful in cases where we are performing regression where some columns need to be transformed for effective modelling.\nFor each column, we can also see the distribution of data represented using histograms with additional information about the target feature embedded in it. Visualizations like these help us in making a tangible narrative and asking questions that might reveal something important. For example, the plot for the average_monthly_hours column shows us that people working the longest hours have the highest percentage of leaving. This might raise questions about the work environent and also the reward system within the company.\nFinally, we can also see the associations/correlations between the features in our dataset. This will help us in removing the redundant features from our dataset when we are creating our ML model. This also helps us in creating a better picture of what is really happening. For instance, number_project is highly correlated with satisfication_level. So, people who are getting more projects to work on are suprisingly less satisfied with their jobs.\n\n\n\nPandas profiling\nPandas profiling is an extension of the describe function in pandas. This library allows us to conduct some quick data analysis on a pandas dataframe. It is very useful for an initial investigation into the mysteries of our dataset.\nLet’s see what we can do with it.\n\nfrom pandas_profiling import ProfileReport\nireport = ProfileReport(df,explorative=True)\nireport.to_notebook_iframe()\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights:\n\nPandas profiling gives a lot of information about each column in our dataset. It also tells us how many zero, negative and infinite values are present in each numerical column. This allows us to think about the preprocessing steps with much more clarity. For instance, negative values in a column can be sign of data corruption. It could also be the case that negative values have a specific meaning in our dataset.\nOn toggling the details of a column, we can see more information like the common and extreme values in a column. Extreme values can be particularly problematic since they can be outliers which make modelling difficult. We might want to remove these outliers so that our ML model can learn and perform better.\nWe can also see the duplicate rows in our data. Duplicate rows can mean a lot of things. Depending on the scenario and how they affect the quality of our analysis we might decide to remove them or keep them. For instance in our example, the duplicate rows represent the employees who have left the company. Since our dataset is imbalanced, these duplicate rows might be an attempt of oversampling in order to improve the balance and create an effective model.\n\nWe have seen some of the insights we can gather using Pandas Profiling. Now, let’s look at a more visual approach to EDA. SweetViz and Pandas profiling give us a lot of numbers to think about, but Autoviz gives us only visualizations to understand our data.\n\n\nAutoViz\nIn his book “Good Charts,” Scott Berinato exclaims: “ A good visualization can communicate the nature and potential impact of information and ideas more powerfully than any other form of communication.”\nAutoViz gives us a lot of different visualizations that can be useful in understanding and communicating the story behind our data. This can be tremendously useful when your audience comprises of people from a non-technical background who don’t understand numbers but like pictures.\nLet’s use some Autoviz.\n\nfrom autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\nsep = ','\ndft = AV.AutoViz(filename=\"\",\n                 sep=sep,\n                 depVar='left',\n                 dfte=df, header=0, verbose=1, \n                 lowess=True, chart_format='jpg',\n                 max_rows_analyzed=150000, max_cols_analyzed=30)\n\nShape of your Data Set: (14999, 10)\n############## C L A S S I F Y I N G  V A R I A B L E S  ####################\nClassifying variables in data set...\n    Number of Numeric Columns =  2\n    Number of Integer-Categorical Columns =  3\n    Number of String-Categorical Columns =  2\n    Number of Factor-Categorical Columns =  0\n    Number of String-Boolean Columns =  0\n    Number of Numeric-Boolean Columns =  2\n    Number of Discrete String Columns =  0\n    Number of NLP String Columns =  0\n    Number of Date Time Columns =  0\n    Number of ID Columns =  0\n    Number of Columns to Delete =  0\n    9 Predictors classified...\n        This does not include the Target column(s)\n        No variables removed since no ID or low-information variables found in data set\n\n################ Binary_Classification VISUALIZATION Started #####################\nData Set Shape: 14999 rows, 10 cols\nData Set columns info:\n* satisfaction_level: 0 nulls, 92 unique vals, most common: {0.1: 358, 0.11: 335}\n* last_evaluation: 0 nulls, 65 unique vals, most common: {0.55: 358, 0.5: 353}\n* number_project: 0 nulls, 6 unique vals, most common: {4: 4365, 3: 4055}\n* average_montly_hours: 0 nulls, 215 unique vals, most common: {156: 153, 135: 153}\n* time_spend_company: 0 nulls, 8 unique vals, most common: {3: 6443, 2: 3244}\n* Work_accident: 0 nulls, 2 unique vals, most common: {0: 12830, 1: 2169}\n* promotion_last_5years: 0 nulls, 2 unique vals, most common: {0: 14680, 1: 319}\n* Department: 0 nulls, 10 unique vals, most common: {'sales': 4140, 'technical': 2720}\n* salary: 0 nulls, 3 unique vals, most common: {'low': 7316, 'medium': 6446}\n* left: 0 nulls, 2 unique vals, most common: {0: 11428, 1: 3571}\n--------------------------------------------------------------------\n   Columns to delete:\n'   []'\n   Boolean variables %s \n\"   ['Work_accident', 'promotion_last_5years']\"\n   Categorical variables %s \n(\"   ['Department', 'salary', 'number_project', 'average_montly_hours', \"\n \"'time_spend_company', 'Work_accident', 'promotion_last_5years']\")\n   Continuous variables %s \n\"   ['satisfaction_level', 'last_evaluation']\"\n   Discrete string variables %s \n'   []'\n   Date and time variables %s \n'   []'\n   ID variables %s \n'   []'\n   Target variable %s \n'   left'\nTotal Number of Scatter Plots = 3\nTime to run AutoViz (in seconds) = 22.712\n\n ###################### VISUALIZATION Completed ########################\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights:\n\nScatter Plots are an important part of EDA. They show us how our data is spread in the 2D/3D plane where the dimensions represent the features from our dataset. From above, we can see how satisfaction level for employees who have left the company differ from the employees who are still working in the company.\nVisualizing the distribution of each feature based on the value of the target variable can also help in spotting important differences. For example, the normed histogram for average_monthly_hours for the people who left the company is very different from the people still working in the company.\nAutoviz also gives us other useful plots like bar charts, correlation heatmap and box plots to understand the patterns hidden in our data. In our example, the bar chart of average satisfaction_level by number_project shows us that people who have worked on the most number of projects (7) have the lowest satisfaction level. On ther other hand, they have the highest average evaluation scores.\n\nNow that we have explored all the three Auto-EDA tools, let’s talk about their utility and limitations."
  },
  {
    "objectID": "posts/2021-05-24-autoeda.html#conclusion",
    "href": "posts/2021-05-24-autoeda.html#conclusion",
    "title": "Exploring Auto-EDA in Python",
    "section": "Conclusion",
    "text": "Conclusion\nEDA lets us understand our data and develop a deeper understanding about the real world. In the previous sections, we have looked at different Auto-EDA tools and noted some insights that can they provide us. Although, these tools are tremendously useful, they have their limitations.\nFirstly, all the EDA tools we have seen above work very well on tabular datasets, but they aren’t useful for datasets from other domains like computer vision, NLP, etc.\nSecondly, they don’t always give us the best information and plots. Autoviz uses some internal heuristics to decide which plots are the most useful, but it is still not clear if these heuristics always work. It is still very hard to answer the question: What information is the most useful for a data scientist?\nFinally, it’s very difficult to configure these tools in a way that is best suited for your data. They are not very flexible. Although, we can customize them to some degree, our problems will always have specific requirements.\nIn conclusion, Auto-EDA is very useful to launch an initial investigation into the data. It gives us the basic information necessary to delve deeper into the data and ask more interesting questions. More importantly, it gives us a way of conducting quick data analysis with much less manual effort.\nI hope to see more such tools that will help data scientists tell better stories through their data.\nSee you on the next adventure!!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Unleashing Productivity - How In-House RAG Systems Can Turbocharge Your Data Science Team\n\n\n\n\n\n\nmarkdown\n\n\ndata_science\n\n\nNLP\n\n\n\n\n\n\n\n\n\nSep 1, 2024\n\n\nMehul Jain and Bishal Agrawal\n\n\n\n\n\n\n\n\n\n\n\n\nMLOps in Action - How to Implement an End-to-End Pipeline for Your Project\n\n\n\n\n\n\nmarkdown\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nMehul Jain\n\n\n\n\n\n\n\n\n\n\n\n\nBreaking Radio Silence - What Have I Been Doing in the Past Few Months\n\n\n\n\n\n\nmarkdown\n\n\n\n\n\n\n\n\n\nMar 26, 2022\n\n\nMehul Jain\n\n\n\n\n\n\n\n\n\n\n\n\nCat Face Mapper - Image Regression with FastAI\n\n\n\n\n\n\nimage_regression\n\n\ndeep_learning\n\n\ntransfer_learning\n\n\n\nIn the blog post, we explore key-point modelling using FastAI and build a small application to test our models.\n\n\n\n\n\nAug 14, 2021\n\n\nMehul Jain\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding graduate admissions - Data driven advice for undergrad students\n\n\n\n\n\n\nmachine_learning\n\n\ndata_science\n\n\n\n\n\n\n\n\n\nJun 9, 2021\n\n\nMehul Jain\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Auto-EDA in Python\n\n\n\n\n\n\nEDA\n\n\ndata_science\n\n\n\nEDA is one of the most important steps in the data science pipeline. In this blog post, we explore automated exploratory data analysis and examine it’s utility.\n\n\n\n\n\nMay 24, 2021\n\n\nMehul Jain\n\n\n\n\n\n\n\n\n\n\n\n\nSmall Data And The Power Of Pre-Trained Models\n\n\n\n\n\n\ndeep_learning\n\n\ntransfer_learning\n\n\n\n\n\n\n\n\n\nMay 3, 2021\n\n\nMehul Jain\n\n\n\n\n\n\n\n\n\n\n\n\nThe essentials of data preprocessing for tabular data\n\n\n\n\n\nIn this blog post, we learn about the fundamentals of preprocessing tabular data.\n\n\n\n\n\nApr 12, 2021\n\n\nMehul Jain\n\n\n\n\n\n\n\n\n\n\n\n\nList Comprehensions - The elegant way of creating lists in python\n\n\n\n\n\nIn this blog post we will explore why we should use list comprehensions and what can we do with them.\n\n\n\n\n\nApr 4, 2021\n\n\nMehul Jain\n\n\n\n\n\n\nNo matching items"
  }
]